{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"images/deep_neural_network.png\" style=\"width:1024px;height:768px;\">",
   "id": "9e1679ef3969ec6e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T20:49:41.147944Z",
     "start_time": "2025-06-06T20:49:41.086465Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.random.seed(1)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize",
   "id": "387c868d79d2eeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Without Dropout",
   "id": "2174a2e3f776acf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:49:42.814736Z",
     "start_time": "2025-06-06T20:49:42.811999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters(layers):\n",
    "    \"\"\"\n",
    "    Wl - weight matrix of shape (layer_dims[l], layer_dims[l-1])\\n\n",
    "    bl - bias vector of shape (layer_dims[l], 1)\n",
    "    :param layer_dims: array (list) containing the dimensions of each layer\n",
    "    :return: dictionary containing the parameters of each layer: \"W1\", \"b1\", ..., \"WL\", \"bL\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layers)\n",
    "    for l in range(1, L):\n",
    "        curr, prev = layers[l], layers[l-1]\n",
    "        dimsP = prev[\"dims\"]\n",
    "        dimsC, activC = curr[\"dims\"], curr[\"activation\"]\n",
    "        # Initialize all weights using \"He\"/\"Xavier\" initialization to eliminate vanishing/exploding gradients\n",
    "        parameters['W' + str(l)] = np.random.randn(dimsC, dimsP)\n",
    "        if activC == \"relu\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(2 / dimsP)\n",
    "        elif activC == \"sigmoid\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(1 / dimsP)\n",
    "        else:\n",
    "            parameters['W' + str(l)] *= 0.01\n",
    "        # Initialize all biases to zero\n",
    "        parameters['b' + str(l)] = np.zeros((dimsC, 1))\n",
    "    return parameters"
   ],
   "id": "5decec3ffeb76cd2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:50:01.651534Z",
     "start_time": "2025-06-06T20:49:49.797642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_parameters = initialize_parameters([\n",
    "\t{\"dims\": 5, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 4, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 3, \"activation\": \"relu\"},\n",
    "])\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [1.02732621, -0.38690873, -0.33404515, -0.67860494, 0.54733184],\n",
    "    [-1.45562088, 1.10351585, -0.48142952, 0.20177804,-0.15771567],\n",
    "    [0.92471825, -1.30294739, -0.20391454, -0.2428973, 0.71705876],\n",
    "    [-0.69563232, -0.10905317, -0.55520641, 0.02669832, 0.36860471]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array(\n",
    "    [0, 0, 0, 0]\n",
    "))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.77825528, 0.8094419, 0.63752091, 0.35531715],\n",
    "    [ 0.63700135, -0.48346861, -0.08689651, -0.66168891],\n",
    "    [-0.18942548, 0.37501795, -0.48907801, -0.28054711]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([[0.]]))"
   ],
   "id": "460e7d313876909e",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      2\u001B[39m t_parameters = initialize_parameters([\n\u001B[32m      3\u001B[39m \t{\u001B[33m\"\u001B[39m\u001B[33mdims\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m5\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mactivation\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mrelu\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m      4\u001B[39m \t{\u001B[33m\"\u001B[39m\u001B[33mdims\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m4\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mactivation\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mrelu\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m      5\u001B[39m \t{\u001B[33m\"\u001B[39m\u001B[33mdims\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m3\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mactivation\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mrelu\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m      6\u001B[39m ])\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m np.allclose(t_parameters[\u001B[33m\"\u001B[39m\u001B[33mW1\u001B[39m\u001B[33m\"\u001B[39m], np.array([\n\u001B[32m      8\u001B[39m     [\u001B[32m1.02732621\u001B[39m, -\u001B[32m0.38690873\u001B[39m, -\u001B[32m0.33404515\u001B[39m, -\u001B[32m0.67860494\u001B[39m, \u001B[32m0.54733184\u001B[39m],\n\u001B[32m      9\u001B[39m     [-\u001B[32m1.45562088\u001B[39m, \u001B[32m1.10351585\u001B[39m, -\u001B[32m0.48142952\u001B[39m, \u001B[32m0.20177804\u001B[39m,-\u001B[32m0.15771567\u001B[39m],\n\u001B[32m     10\u001B[39m     [\u001B[32m0.92471825\u001B[39m, -\u001B[32m1.30294739\u001B[39m, -\u001B[32m0.20391454\u001B[39m, -\u001B[32m0.2428973\u001B[39m, \u001B[32m0.71705876\u001B[39m],\n\u001B[32m     11\u001B[39m     [-\u001B[32m0.69563232\u001B[39m, -\u001B[32m0.10905317\u001B[39m, -\u001B[32m0.55520641\u001B[39m, \u001B[32m0.02669832\u001B[39m, \u001B[32m0.36860471\u001B[39m]\n\u001B[32m     12\u001B[39m ]))\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[43mnp\u001B[49m.allclose(t_parameters[\u001B[33m\"\u001B[39m\u001B[33mb1\u001B[39m\u001B[33m\"\u001B[39m], np.array(\n\u001B[32m     14\u001B[39m     [\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m]\n\u001B[32m     15\u001B[39m ))\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m np.allclose(t_parameters[\u001B[33m\"\u001B[39m\u001B[33mW2\u001B[39m\u001B[33m\"\u001B[39m], np.array([\n\u001B[32m     17\u001B[39m     [-\u001B[32m0.77825528\u001B[39m, \u001B[32m0.8094419\u001B[39m, \u001B[32m0.63752091\u001B[39m, \u001B[32m0.35531715\u001B[39m],\n\u001B[32m     18\u001B[39m     [ \u001B[32m0.63700135\u001B[39m, -\u001B[32m0.48346861\u001B[39m, -\u001B[32m0.08689651\u001B[39m, -\u001B[32m0.66168891\u001B[39m],\n\u001B[32m     19\u001B[39m     [-\u001B[32m0.18942548\u001B[39m, \u001B[32m0.37501795\u001B[39m, -\u001B[32m0.48907801\u001B[39m, -\u001B[32m0.28054711\u001B[39m]\n\u001B[32m     20\u001B[39m ]))\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m np.allclose(t_parameters[\u001B[33m\"\u001B[39m\u001B[33mb2\u001B[39m\u001B[33m\"\u001B[39m], np.array([[\u001B[32m0.\u001B[39m]]))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_cython.pyx:1187\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_cython.pyx:627\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_cython.pyx:1103\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_cython.pyx:1061\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/jupyter_debug/pydev_jupyter_plugin.py:171\u001B[39m, in \u001B[36mstop\u001B[39m\u001B[34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[39m\n\u001B[32m    169\u001B[39m     frame = suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[32m    170\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m         \u001B[43mmain_debugger\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    172\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    173\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1217\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1220\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1232\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1234\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1235\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1239\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## With Dropout",
   "id": "401e3180155cc4fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:31:53.192632Z",
     "start_time": "2025-06-06T20:31:53.190211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters_dropout(layers):\n",
    "    parameters = {}\n",
    "    L = len(layers)\n",
    "    for l in range(1, L):\n",
    "        curr, prev = layers[l], layers[l-1]\n",
    "        dimsP = prev[\"dims\"]\n",
    "        dimsC, keep_probsC, activC = curr[\"dims\"], curr[\"keep_prob\"], curr[\"activation\"]\n",
    "        parameters['W' + str(l)] = np.random.randn(dimsC, dimsP)\n",
    "        if activC == \"relu\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(2 / dimsP)\n",
    "        elif activC == \"sigmoid\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(1 / dimsP)\n",
    "        else:\n",
    "            parameters['W' + str(l)] *= 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((dimsC, 1))\n",
    "        parameters['keep_prob' + str(l)] = keep_probsC\n",
    "    return parameters"
   ],
   "id": "c920cbf74bd7962",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Forward propagation",
   "id": "91c3fdd1722d0279"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "ab4e8611d1b4b61c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:31:57.736714Z",
     "start_time": "2025-06-06T20:31:57.735072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation\n",
    "    :param Z: numpy array of any shape\n",
    "    :return: output of sigmoid(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "b5db74f340bce00a",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:00.065361Z",
     "start_time": "2025-06-06T20:32:00.063670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function\n",
    "    :param Z: Output of the linear layer, of any shape\n",
    "    :return:  output of relu(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "a41ba41f7c9ef5c8",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:01.377715Z",
     "start_time": "2025-06-06T20:32:01.375927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "    :param A: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :return:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter\n",
    "        cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ],
   "id": "d034fedf9e3fa68e",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:02.747046Z",
     "start_time": "2025-06-06T20:32:02.745135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
    "assert np.allclose(t_Z, np.array([3.26295337, -1.23429987]))"
   ],
   "id": "b79103a0e69e3f92",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Forward (without dropout)",
   "id": "7ad1580d4c488c49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:04.272154Z",
     "start_time": "2025-06-06T20:32:04.270072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :param activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    :return:\n",
    "        A -- the output of the activation function, also called the post-activation value\n",
    "        cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "    \"\"\"\n",
    "    A, linear_cache, activation_cache = None, None, None\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ],
   "id": "d3050cf481e6c9ca",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:05.980831Z",
     "start_time": "2025-06-06T20:32:05.978566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A_prev = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n",
    "assert np.allclose(t_A, np.array([[0.96313579, 0.22542973]]));\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n",
    "assert np.allclose(t_A, np.array([[3.26295337, 0.0]]));"
   ],
   "id": "a443fec0626799d8",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:07.372799Z",
     "start_time": "2025-06-06T20:32:07.370641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_propagate(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # Number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR->RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "id": "8484174c6a412fca",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:08.942581Z",
     "start_time": "2025-06-06T20:32:08.940042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_X = np.random.randn(5,4)\n",
    "t_parameters = {\n",
    "    \"W1\": np.random.randn(4,5),\n",
    "    \"b1\": np.random.randn(4,1),\n",
    "    \"W2\": np.random.randn(3,4),\n",
    "    \"b2\": np.random.randn(3,1),\n",
    "    \"W3\": np.random.randn(1,3),\n",
    "    \"b3\": np.random.randn(1,1)\n",
    "}\n",
    "t_AL, t_caches = forward_propagate(t_X, t_parameters)\n",
    "print(\"AL = \" + str(t_AL))\n",
    "assert np.allclose(t_AL, np.array([[0.77634609, 0.9998399, 0.99021857, 0.33755508]]))"
   ],
   "id": "a23e169e9445e216",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.77634609 0.9998399  0.99021857 0.33755508]]\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Forward (with dropout)",
   "id": "ecebe8fd6aae9f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:12.391120Z",
     "start_time": "2025-06-06T20:32:12.388893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward_dropout(A_prev, W, b, keep_prob, activation):\n",
    "    A, linear_cache, activation_cache = None, None, None\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    D = None\n",
    "    if keep_prob < 1.0:\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_prob).astype(int)\n",
    "        A = np.multiply(A, D)\n",
    "        A = A / keep_prob\n",
    "\n",
    "    cache = (linear_cache, activation_cache, D)\n",
    "    return A, cache"
   ],
   "id": "a5d60902cb51f400",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:13.931810Z",
     "start_time": "2025-06-06T20:32:13.929207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_propagate_dropout(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 3 # Number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR->RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W, b, keep_prob = parameters['W' + str(l)], parameters['b' + str(l)], parameters['keep_prob' + str(l)]\n",
    "        A, cache = linear_activation_forward_dropout(A_prev, W, b, keep_prob, activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    WL, bL, keep_probL = parameters['W' + str(L)], parameters['b' + str(L)], parameters['keep_prob' + str(L)]\n",
    "    AL, cache = linear_activation_forward_dropout(A, WL, bL, keep_probL, activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "id": "50f636cdc178e556",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cost function",
   "id": "721ae584741bcf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:15.825304Z",
     "start_time": "2025-06-06T20:32:15.823283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    :param AL: probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "    :param Y: true \"label\" vector\n",
    "    :return: cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ],
   "id": "b1ce9a75e8dcee78",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:17.137150Z",
     "start_time": "2025-06-06T20:32:17.135260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t_Y = np.asarray([[1, 1, 0]])\n",
    "t_AL = np.array([[0.8, 0.9, 0.4]])\n",
    "t_cost = compute_cost(t_AL, t_Y)\n",
    "assert np.allclose(t_cost, 0.2797765635793422)"
   ],
   "id": "10f4fd829be9e49f",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Backward propagation",
   "id": "10414c6d1d6d8a2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "9e99ffd08853a793"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:18.527485Z",
     "start_time": "2025-06-06T20:32:18.525715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ],
   "id": "d111bb470b32c5c3",
   "outputs": [],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:21.611571Z",
     "start_time": "2025-06-06T20:32:21.609803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ],
   "id": "f683bb6d2bbd068e",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:23.637776Z",
     "start_time": "2025-06-06T20:32:23.635783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ],
   "id": "327d27e19bd420c1",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:26.186806Z",
     "start_time": "2025-06-06T20:32:26.184062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_dZ = np.random.randn(3,4)\n",
    "t_linear_cache = (np.random.randn(5,4), np.random.randn(3,5), np.random.randn(3,1))\n",
    "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [-1.15171336, 0.06718465, -0.3204696, 2.09812712],\n",
    "    [ 0.60345879, -3.72508701, 5.81700741, -3.84326836],\n",
    "    [-0.4319552, -1.30987417, 1.72354705, 0.05070578],\n",
    "    [-0.38981415, 0.60811244, -1.25938424, 1.47191593],\n",
    "    [-2.52214926, 2.67882552, -0.67947465, 1.48119548],\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.07313866, -0.0976715, -0.87585828, 0.73763362, 0.00785716],\n",
    "    [0.85508818, 0.37530413, -0.59912655, 0.71278189, -0.58931808],\n",
    "    [0.97913304, -0.24376494, -0.08839671, 0.55151192, -0.10290907],\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.14713786],\n",
    "    [-0.11313155],\n",
    "    [-0.13209101]\n",
    "]))"
   ],
   "id": "94ce7c376473b2d4",
   "outputs": [],
   "execution_count": 179
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Backward (no dropout)",
   "id": "ada9cbaa5036fbd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:30.843914Z",
     "start_time": "2025-06-06T20:32:30.841318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward_dropout(dA, cache, activation):\n",
    "    linear_cache, activation_cache, _ = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ],
   "id": "dff6636099c9c902",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:32.628887Z",
     "start_time": "2025-06-06T20:32:32.625753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_dAL = np.random.randn(1,2)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z = np.random.randn(1,2)\n",
    "t_linear_activation_cache = ((t_A, t_W, t_b), t_Z)\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.11017994, 0.01105339],\n",
    "    [0.09466817, 0.00949723],\n",
    "    [-0.05743092, -0.00576154]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.10266786, 0.09778551, -0.01968084]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.05729622]\n",
    "]))\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.44090989, 0.0],\n",
    "    [0.37883606, 0.0],\n",
    "    [-0.2298228, 0.0]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.44513824, 0.37371418, -0.10478989]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.20837892]\n",
    "]))"
   ],
   "id": "f55b269302607faf",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:35.586609Z",
     "start_time": "2025-06-06T20:32:35.583975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_propagate(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "    return grads"
   ],
   "id": "fecaa2dc24d5e936",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:37.743409Z",
     "start_time": "2025-06-06T20:32:37.739869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "t_AL = np.random.randn(1, 2)\n",
    "t_Y = np.array([[1, 0]])\n",
    "\n",
    "t_A1 = np.random.randn(4, 2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_Z1 = np.random.randn(3,2)\n",
    "t_linear_cache_activation1 = ((t_A1, t_W1, t_b1), t_Z1)\n",
    "\n",
    "t_A2 = np.random.randn(3,2)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_Z2 = np.random.randn(1,2)\n",
    "t_linear_cache_activation2 = ((t_A2, t_W2, t_b2), t_Z2)\n",
    "\n",
    "t_AL, t_Y_assess, t_caches = t_AL, t_Y, (t_linear_cache_activation1, t_linear_cache_activation2)\n",
    "t_grads = backward_propagate(t_AL, t_Y_assess, t_caches)\n",
    "\n",
    "assert np.allclose(t_grads['dA0'], np.array([\n",
    "    [0.0, 0.52257901],\n",
    "    [0.0, -0.3269206],\n",
    "    [0.0, -0.32070404],\n",
    "    [0.0, -0.74079187]\n",
    "]))\n",
    "assert np.allclose(t_grads['dA1'], np.array([\n",
    "    [0.12913162, -0.44014127],\n",
    "    [-0.14175655, 0.48317296],\n",
    "    [0.01663708, -0.05670698]\n",
    "]))\n",
    "assert np.allclose(t_grads['dW1'], np.array([\n",
    "    [0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.05283652, 0.01005865, 0.01777766, 0.0135308 ],\n",
    "]))\n",
    "assert np.allclose(t_grads['dW2'], np.array([\n",
    "    [-0.39202432, -0.13325855, -0.04601089]\n",
    "]))\n",
    "assert np.allclose(t_grads['db1'], np.array([\n",
    "    [-0.22007063],\n",
    "    [0.0],\n",
    "    [-0.02835349]\n",
    "]))\n",
    "assert np.allclose(t_grads['db2'], np.array([\n",
    "    [0.15187861]\n",
    "]))"
   ],
   "id": "d677715e675b67b9",
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Backward (with dropout)",
   "id": "1e0eecefdecee41f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:41.521061Z",
     "start_time": "2025-06-06T20:32:41.517960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_propagate_dropout(AL, Y, parameters, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    keep_probL = parameters[\"keep_prob\" + str(L)]\n",
    "    if keep_probL < 1.0:\n",
    "        _, _, D = caches[L-1]\n",
    "        dAL = np.multiply(dAL, D)\n",
    "        dAL = dAL / keep_probL\n",
    "\n",
    "    curr_cache = caches[L-1]\n",
    "    dA_prev, dW, db = linear_activation_backward_dropout(dAL, curr_cache, activation = \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = (dA_prev, dW, db)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        curr_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward_dropout(grads[\"dA\" + str(l+1)], curr_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = (dA_prev, dW, db)\n",
    "\n",
    "    for l in reversed(range(1, L)):\n",
    "        keep_prob = parameters[\"keep_prob\" + str(l)]\n",
    "        if keep_prob < 1.0:\n",
    "            _, _, D = caches[l-1]\n",
    "            dAl = grads[\"dA\" + str(l)]\n",
    "            dAl = np.multiply(dAl, D)\n",
    "            dAl = dAl / keep_prob\n",
    "            grads[\"dA\" + str(l)] = dAl\n",
    "\n",
    "    return grads"
   ],
   "id": "df839358373dd610",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Update parameters",
   "id": "4cfc915e5aea7a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Update parameters (without dropout)",
   "id": "ff8136673234adac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:45.124733Z",
     "start_time": "2025-06-06T20:32:45.122645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ],
   "id": "d26002ab614d98b1",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:47.033473Z",
     "start_time": "2025-06-06T20:32:47.030300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_parameters = {\"W1\": t_W1,\n",
    "                \"b1\": t_b1,\n",
    "                \"W2\": t_W2,\n",
    "                \"b2\": t_b2}\n",
    "\n",
    "np.random.seed(3)\n",
    "t_dW1 = np.random.randn(3,4)\n",
    "t_db1 = np.random.randn(3,1)\n",
    "t_dW2 = np.random.randn(1,3)\n",
    "t_db2 = np.random.randn(1,1)\n",
    "t_grads = {\"dW1\": t_dW1,\n",
    "           \"db1\": t_db1,\n",
    "           \"dW2\": t_dW2,\n",
    "           \"db2\": t_db2}\n",
    "t_parameters = update_parameters(t_parameters, t_grads, 0.1)\n",
    "\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [-0.59562069, -0.09991781, -2.14584584, 1.82662008],\n",
    "    [-1.76569676, -0.80627147, 0.51115557, -1.18258802],\n",
    "    [-1.0535704, -0.86128581, 0.68284052, 2.20374577]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array([\n",
    "    [-0.04659241],\n",
    "    [-1.28888275],\n",
    "    [0.53405496]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.55569196, 0.0354055, 1.32964895]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([\n",
    "    [-0.84610769]\n",
    "]))"
   ],
   "id": "fd838eaa60909e51",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Update parameters (with dropout)",
   "id": "b6e944017016fad1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:50.636625Z",
     "start_time": "2025-06-06T20:32:50.634571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters_dropout(params, grads, learning_rate):\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 3\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ],
   "id": "b173d8883177264b",
   "outputs": [],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "a0b255e0e4bb1326"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:53.250114Z",
     "start_time": "2025-06-06T20:32:53.248581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "from common import CV_DATA_DIR"
   ],
   "id": "85f1d7647f1c1549",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:54.407464Z",
     "start_time": "2025-06-06T20:32:54.404638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"train_catvnoncat.h5\", \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"test_catvnoncat.h5\", \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ],
   "id": "b47814d4d1ea5ddc",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:56.245978Z",
     "start_time": "2025-06-06T20:32:56.242583Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()",
   "id": "8b0fac50f4cdd9e9",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:57.028906Z",
     "start_time": "2025-06-06T20:32:57.026587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print (\"Number of training examples:\", train_x_orig.shape[0])\n",
    "print (\"Number of testing examples:\", test_x_orig.shape[0])\n",
    "print (\"Each image is of size: (\" + str(train_x_orig.shape[1]) + \", \" + str(train_x_orig.shape[1]) + \", 3)\")\n",
    "\n",
    "assert train_x_orig.shape == (209, 64, 64, 3)\n",
    "assert train_y.shape == (1, 209)\n",
    "assert test_x_orig.shape == (50, 64, 64, 3)\n",
    "assert test_y.shape == (1, 50)"
   ],
   "id": "a0357974981bd6fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n"
     ]
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:32:58.271155Z",
     "start_time": "2025-06-06T20:32:58.267416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print(\"train_x's shape: \" + str(train_x.shape))\n",
    "print(\"test_x's shape: \" + str(test_x.shape))"
   ],
   "id": "fb1cacb5af286809",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 192
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train L-layer model",
   "id": "9dd9c9e0bbc54fd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train L-layer Model (without dropout)",
   "id": "9ef987cdffdad827"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:38:17.216100Z",
     "start_time": "2025-06-06T20:38:17.214314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layers_config = [\n",
    "\t{\"dims\": 12288, \"activation\": None},\n",
    "\t{\"dims\": 20, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 7, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 5, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "id": "5ec28d71255f185a",
   "outputs": [],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:34:12.002345Z",
     "start_time": "2025-06-06T20:34:12.000159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def L_layer_model(X, Y, layers, learning_rate = 0.0075, num_iterations = 3500, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters(layers)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward_propagate(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagate(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ],
   "id": "2557736fb8cfb5c1",
   "outputs": [],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:34:42.179731Z",
     "start_time": "2025-06-06T20:34:13.079184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters, costs = L_layer_model(\n",
    "    train_x, train_y,\n",
    "    layers_config,\n",
    "    num_iterations = 3500,\n",
    "    print_cost = True)"
   ],
   "id": "d1bb96e07147bb37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.0081445907083524\n",
      "Cost after iteration 100: 0.6759653401043519\n",
      "Cost after iteration 200: 0.664907102637664\n",
      "Cost after iteration 300: 0.657556288283337\n",
      "Cost after iteration 400: 0.6526602719800702\n",
      "Cost after iteration 500: 0.649238912006134\n",
      "Cost after iteration 600: 0.6467162744881768\n",
      "Cost after iteration 700: 0.6446557808185854\n",
      "Cost after iteration 800: 0.6425085221335521\n",
      "Cost after iteration 900: 0.6403881228483663\n",
      "Cost after iteration 1000: 0.6384291123880995\n",
      "Cost after iteration 1100: 0.6364439655178741\n",
      "Cost after iteration 1200: 0.6340904612892516\n",
      "Cost after iteration 1300: 0.6315491637520989\n",
      "Cost after iteration 1400: 0.628492444199776\n",
      "Cost after iteration 1500: 0.6247493591369389\n",
      "Cost after iteration 1600: 0.6200676283988742\n",
      "Cost after iteration 1700: 0.6137058237143043\n",
      "Cost after iteration 1800: 0.6073967042509079\n",
      "Cost after iteration 1900: 0.5994876506445569\n",
      "Cost after iteration 2000: 0.5939265919333917\n",
      "Cost after iteration 2100: 0.586019877610374\n",
      "Cost after iteration 2200: 0.5650884448918425\n",
      "Cost after iteration 2300: 0.5318456560854982\n",
      "Cost after iteration 2400: 0.47200847095245146\n",
      "Cost after iteration 2500: 0.36752167715734446\n",
      "Cost after iteration 2600: 0.3225014802376668\n",
      "Cost after iteration 2700: 0.5710106736938716\n",
      "Cost after iteration 2800: 0.1371837857839284\n",
      "Cost after iteration 2900: 0.08764067335259705\n",
      "Cost after iteration 3000: 0.4466243783670908\n",
      "Cost after iteration 3100: 0.16462506862745602\n",
      "Cost after iteration 3200: 0.23433875197320314\n",
      "Cost after iteration 3300: 0.07399694473746907\n",
      "Cost after iteration 3400: 0.05305529615791699\n",
      "Cost after iteration 3499: 0.04324467724541572\n"
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train L-layer Model (with dropout)",
   "id": "988e7967aef48e2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:37:51.465884Z",
     "start_time": "2025-06-06T20:37:51.464195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layers_config_dropout = [\n",
    "\t{\"dims\": 12288, \"keep_prob\": 1.0, \"activation\": None},\n",
    "\t{\"dims\": 20, \"keep_prob\": 0.95, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 7, \"keep_prob\": 1.0, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 5, \"keep_prob\": 1.0, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 1, \"keep_prob\": 1.0, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "id": "3d6522f3cc982d2d",
   "outputs": [],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:33:41.010171Z",
     "start_time": "2025-06-06T20:33:41.007880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def L_layer_model_dropout(X, Y, layers, learning_rate = 0.0075, num_iterations = 3500, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters_dropout(layers)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward_propagate_dropout(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagate_dropout(AL, Y, parameters, caches)\n",
    "        parameters = update_parameters_dropout(parameters, grads, learning_rate)\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ],
   "id": "5e59dfdc08c6b530",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:37:45.795547Z",
     "start_time": "2025-06-06T20:37:29.286569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters_dropout, costs_dropout = L_layer_model_dropout(\n",
    "    train_x, train_y,\n",
    "    layers_config_dropout,\n",
    "    num_iterations = 3500,\n",
    "    print_cost = True)"
   ],
   "id": "aa81b270d7de26d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.0244339515794885\n",
      "Cost after iteration 100: 0.6755486893076124\n",
      "Cost after iteration 200: 0.6662339753121558\n",
      "Cost after iteration 300: 0.6569395148096703\n",
      "Cost after iteration 400: 0.651914658104107\n",
      "Cost after iteration 500: 0.6469559665261049\n",
      "Cost after iteration 600: 0.6454436182841561\n",
      "Cost after iteration 700: 0.6425693748162455\n",
      "Cost after iteration 800: 0.6377424156581565\n",
      "Cost after iteration 900: 0.6369706491594286\n",
      "Cost after iteration 1000: 0.6329118735794748\n",
      "Cost after iteration 1100: 0.633950429847234\n",
      "Cost after iteration 1200: 0.6303781166830383\n",
      "Cost after iteration 1300: 0.6256813271658227\n",
      "Cost after iteration 1400: 0.6226756263327053\n",
      "Cost after iteration 1500: 0.6142971271598081\n",
      "Cost after iteration 1600: 0.6086507395692743\n",
      "Cost after iteration 1700: 0.6023532123030833\n",
      "Cost after iteration 1800: 0.596215118188869\n",
      "Cost after iteration 1900: 0.5854860998374658\n",
      "Cost after iteration 2000: 0.573452206840469\n",
      "Cost after iteration 2100: 0.5654662006660257\n",
      "Cost after iteration 2200: 0.5468138480194282\n",
      "Cost after iteration 2300: 0.5337355836421454\n",
      "Cost after iteration 2400: 0.5350324820152247\n",
      "Cost after iteration 2500: 0.5141678784779519\n",
      "Cost after iteration 2600: 0.4991538077179012\n",
      "Cost after iteration 2700: 0.4771185056605417\n",
      "Cost after iteration 2800: 0.4465530023136864\n",
      "Cost after iteration 2900: 0.4399565817662688\n",
      "Cost after iteration 3000: 0.4164295421520268\n",
      "Cost after iteration 3100: 0.40058523610399777\n",
      "Cost after iteration 3200: 0.36798522367247566\n",
      "Cost after iteration 3300: 0.3598817807977258\n",
      "Cost after iteration 3400: 0.21179005675305013\n",
      "Cost after iteration 3499: 0.16021558096876376\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "f0f703bd86e86840"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predict (with dropout)",
   "id": "215745db1dc064e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:34:56.697575Z",
     "start_time": "2025-06-06T20:34:56.695456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = forward_propagate(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    accuracy = np.sum((p == y) / m)\n",
    "    return p, accuracy"
   ],
   "id": "b8c62dcbe48c36c5",
   "outputs": [],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:36:11.213943Z",
     "start_time": "2025-06-06T20:36:11.211094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_, accuracy = predict(train_x, train_y, parameters)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "assert np.greater(accuracy, 0.9)"
   ],
   "id": "4da2e9c768b9542c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9856459330143538\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predict (without dropout)",
   "id": "a25962460149e6bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:35:16.181167Z",
     "start_time": "2025-06-06T20:35:16.179029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_dropout(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = forward_propagate_dropout(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    accuracy = np.sum((p == y) / m)\n",
    "    return p, accuracy"
   ],
   "id": "1fcb122c98491911",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:36:33.623333Z",
     "start_time": "2025-06-06T20:36:33.620388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_, accuracy_dropout = predict_dropout(train_x, train_y, parameters_dropout)\n",
    "print(\"Accuracy:\", accuracy_dropout)\n",
    "assert np.greater(accuracy_dropout, 0.9)"
   ],
   "id": "857f963c1177a086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9521531100478466\n"
     ]
    }
   ],
   "execution_count": 214
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Checking\n",
    "\n",
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and loss function.\n",
    "\n",
    "Definition of a derivative (or gradient):$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "For each i in parameters:\n",
    "- Compute `J_p[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(param_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagate(X, vec_to_dic(`$\\theta^{+}$`, param_shapes))`.\n",
    "- To compute `J_m[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Where gradapprox[i] is an approximation of the gradient with respect to `param_values[i]`. Then comparing gradapprox vector with the gradients vector from backpropagation the difference can be received:\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n",
    "\n",
    "$10^{-7}$ or less is good, $10^{-5}$ need to double-check components, $10^{-3}$ - need to worry."
   ],
   "id": "17ec7d639912c1ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:36:37.147397Z",
     "start_time": "2025-06-06T20:36:37.145178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dic_to_vec(parameters):\n",
    "    theta = []\n",
    "    shapes = {}\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]:\n",
    "        shapes[key] = parameters[key].shape\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta, shapes"
   ],
   "id": "1d510081dcbae4",
   "outputs": [],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:36:38.234806Z",
     "start_time": "2025-06-06T20:36:38.232940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vec_to_dic(param_values, param_shapes):\n",
    "    parameters = {}\n",
    "    offset = 0\n",
    "    for key,shape in param_shapes.items():\n",
    "        items = shape[0] * shape[1]\n",
    "        parameters[key] = param_values[offset:offset+items].reshape(shape)\n",
    "        offset += items\n",
    "    return parameters"
   ],
   "id": "c0a9985375c85326",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:36:39.244292Z",
     "start_time": "2025-06-06T20:36:39.242208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def grad_to_vec(gradients):\n",
    "    theta = []\n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\", \"dW4\", \"db4\"]:\n",
    "        new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count += 1\n",
    "    return theta"
   ],
   "id": "27bf93c91c41ec5d",
   "outputs": [],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:36:42.340959Z",
     "start_time": "2025-06-06T20:36:42.338242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_check(parameters, gradients, X, Y, N=100, epsilon=1e-7):\n",
    "    param_values, param_shapes = dic_to_vec(parameters)\n",
    "    grads = grad_to_vec(gradients)\n",
    "\n",
    "    No = param_values.shape[0]\n",
    "    J_p = np.zeros((No, 1))\n",
    "    J_m = np.zeros((No, 1))\n",
    "    gradapproxs = np.zeros((No, 1))\n",
    "\n",
    "    assert N <= No, \\\n",
    "        \"N parameter is invalid\"\n",
    "\n",
    "    for i in range(N):\n",
    "        theta_p = np.copy(param_values)\n",
    "        theta_p[i] = theta_p[i] + epsilon\n",
    "        Yp_hat, _ = forward_propagate(X, vec_to_dic(theta_p, param_shapes))\n",
    "        J_p[i] = compute_cost(Yp_hat, Y)\n",
    "\n",
    "        theta_m = np.copy(param_values)\n",
    "        theta_m[i] = theta_m[i] - epsilon\n",
    "        Ym_hat, _ = forward_propagate(X, vec_to_dic(theta_m, param_shapes))\n",
    "        J_m[i] = compute_cost(Ym_hat, Y)\n",
    "\n",
    "        gradapproxs[i] = (J_p[i] - J_m[i]) / (2 * epsilon)\n",
    "\n",
    "    num = np.linalg.norm(grads[:N] - gradapproxs[:N])\n",
    "    den = np.linalg.norm(grads[:N]) + np.linalg.norm(gradapproxs[:N])\n",
    "    differences = num / den\n",
    "    return differences\n"
   ],
   "id": "3258e01cfaf240e2",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T20:38:38.496131Z",
     "start_time": "2025-06-06T20:38:38.306388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters = initialize_parameters(layers_config)\n",
    "AL, caches = forward_propagate(train_x, parameters)\n",
    "gradients = backward_propagate(AL, train_y, caches)\n",
    "\n",
    "difference = gradient_check(parameters, gradients, train_x, train_y)\n",
    "print(\"Difference:\", difference)"
   ],
   "id": "ad3bb3ae1f82e8bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 6.000657127924237e-07\n"
     ]
    }
   ],
   "execution_count": 225
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
