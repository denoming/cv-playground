{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"images/deep_neural_network.png\" style=\"width:1024px;height:768px;\">",
   "id": "9e1679ef3969ec6e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ],
   "id": "c06b333a6035f70c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize",
   "id": "387c868d79d2eeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Without Dropout",
   "id": "2174a2e3f776acf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initialize_parameters(layers_config):\n",
    "    \"\"\"\n",
    "    Wl - weight matrix of shape (layer_dims[l], layer_dims[l-1])\\n\n",
    "    bl - bias vector of shape (layer_dims[l], 1)\n",
    "    :param layer_dims: array (list) containing the dimensions of each layer\n",
    "    :return: dictionary containing the parameters of each layer: \"W1\", \"b1\", ..., \"WL\", \"bL\n",
    "    \"\"\"\n",
    "    L = len(layers_config) - 1 # Number of hidden layers and output layer\n",
    "    parameters = {\"L\": L}\n",
    "    for l in range(1, L+1):\n",
    "        curr, prev = layers_config[l], layers_config[l-1]\n",
    "        dimsP = prev[\"dims\"]\n",
    "        dimsC, activC = curr[\"dims\"], curr[\"activation\"]\n",
    "\n",
    "        # Initialize all weights using \"He\"/\"Xavier\" initialization to eliminate vanishing/exploding gradients\n",
    "        if activC == \"relu\":\n",
    "            parameters['W' + str(l)] = np.random.randn(dimsC, dimsP) * np.sqrt(2 / dimsP)\n",
    "        elif activC == \"sigmoid\":\n",
    "            parameters['W' + str(l)] = np.random.randn(dimsC, dimsP) * np.sqrt(1 / dimsP)\n",
    "        else:\n",
    "            parameters['W' + str(l)] *= 0.01\n",
    "\n",
    "        # Initialize all biases to zero\n",
    "        parameters['b' + str(l)] = np.zeros((dimsC, 1))\n",
    "    return parameters"
   ],
   "id": "5decec3ffeb76cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_parameters = initialize_parameters([\n",
    "\t{\"dims\": 5, \"activation\": \"none\"},\n",
    "\t{\"dims\": 4, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 3, \"activation\": \"relu\"},\n",
    "])\n",
    "\n",
    "print(t_parameters[\"W1\"])\n",
    "print(t_parameters[\"W2\"])\n",
    "print(t_parameters[\"b1\"])\n",
    "print(t_parameters[\"b2\"])\n",
    "\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [1.02732621, -0.38690873, -0.33404515, -0.67860494, 0.54733184],\n",
    "    [-1.45562088, 1.10351585, -0.48142952, 0.20177804,-0.15771567],\n",
    "    [0.92471825, -1.30294739, -0.20391454, -0.2428973, 0.71705876],\n",
    "    [-0.69563232, -0.10905317, -0.55520641, 0.02669832, 0.36860471]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array(\n",
    "    [0, 0, 0, 0]\n",
    "))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.77825528, 0.8094419, 0.63752091, 0.35531715],\n",
    "    [ 0.63700135, -0.48346861, -0.08689651, -0.66168891],\n",
    "    [-0.18942548, 0.37501795, -0.48907801, -0.28054711]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([[0.]]))"
   ],
   "id": "460e7d313876909e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## With Dropout",
   "id": "401e3180155cc4fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initialize_parameters_dropout(layers_config):\n",
    "    L = len(layers_config) - 1 # Number of hidden layers and output layer\n",
    "    parameters = {\"L\": L}\n",
    "    for l in range(1, L+1):\n",
    "        curr, prev = layers_config[l], layers_config[l-1]\n",
    "        dimsP = prev[\"dims\"]\n",
    "        dimsC, keep_probC, activC = curr[\"dims\"], curr[\"keep_prob\"], curr[\"activation\"]\n",
    "\n",
    "        # Initialize all weights using \"He\"/\"Xavier\" initialization to eliminate vanishing/exploding gradients\n",
    "        if activC == \"relu\":\n",
    "            parameters['W' + str(l)] = np.random.randn(dimsC, dimsP) * np.sqrt(2 / dimsP)\n",
    "        elif activC == \"sigmoid\":\n",
    "            parameters['W' + str(l)] = np.random.randn(dimsC, dimsP) * np.sqrt(1 / dimsP)\n",
    "        else:\n",
    "            parameters['W' + str(l)] *= 0.01\n",
    "\n",
    "        # Initialize all biases to zero\n",
    "        parameters['b' + str(l)] = np.zeros((dimsC, 1))\n",
    "        # Initialize keep probability\n",
    "        parameters['keep_prob' + str(l)] = keep_probC\n",
    "    return parameters"
   ],
   "id": "c920cbf74bd7962",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Forward propagation",
   "id": "91c3fdd1722d0279"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "ab4e8611d1b4b61c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation\n",
    "    :param Z: numpy array of any shape\n",
    "    :return: output of sigmoid(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "b5db74f340bce00a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function\n",
    "    :param Z: Output of the linear layer, of any shape\n",
    "    :return:  output of relu(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "a41ba41f7c9ef5c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "    :param A: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :return:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter\n",
    "        cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ],
   "id": "d034fedf9e3fa68e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
    "assert np.allclose(t_Z, np.array([3.26295337, -1.23429987]))"
   ],
   "id": "b79103a0e69e3f92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Forward (without dropout)",
   "id": "7ad1580d4c488c49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :param activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    :return:\n",
    "        A -- the output of the activation function, also called the post-activation value\n",
    "        cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "    \"\"\"\n",
    "    A, linear_cache, activation_cache = None, None, None\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ],
   "id": "d3050cf481e6c9ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A_prev = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n",
    "assert np.allclose(t_A, np.array([[0.96313579, 0.22542973]]));\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n",
    "assert np.allclose(t_A, np.array([[3.26295337, 0.0]]));"
   ],
   "id": "a443fec0626799d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_propagate(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = parameters[\"L\"] # Number of layers in the NN\n",
    "\n",
    "    # Implement [LINEAR->RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "id": "8484174c6a412fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_X = np.random.randn(5,4)\n",
    "t_parameters = {\n",
    "    \"L\": 3,\n",
    "    \"W1\": np.random.randn(4,5),\n",
    "    \"b1\": np.random.randn(4,1),\n",
    "    \"W2\": np.random.randn(3,4),\n",
    "    \"b2\": np.random.randn(3,1),\n",
    "    \"W3\": np.random.randn(1,3),\n",
    "    \"b3\": np.random.randn(1,1)\n",
    "}\n",
    "t_AL, t_caches = forward_propagate(t_X, t_parameters)\n",
    "print(\"AL = \" + str(t_AL))\n",
    "assert np.allclose(t_AL, np.array([[0.77634609, 0.9998399, 0.99021857, 0.33755508]]))"
   ],
   "id": "a23e169e9445e216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Forward (with dropout)",
   "id": "ecebe8fd6aae9f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_activation_forward_dropout(A_prev, W, b, keep_prob, activation):\n",
    "    A, linear_cache, activation_cache = None, None, None\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    D = None\n",
    "    if keep_prob < 1.0:\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_prob).astype(int)\n",
    "        A = np.multiply(A, D)\n",
    "        A = A / keep_prob\n",
    "\n",
    "    cache = (linear_cache, activation_cache, D)\n",
    "    return A, cache"
   ],
   "id": "a5d60902cb51f400",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_propagate_dropout(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = parameters[\"L\"] # Number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR->RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W, b, keep_prob = parameters['W' + str(l)], parameters['b' + str(l)], parameters['keep_prob' + str(l)]\n",
    "        A, cache = linear_activation_forward_dropout(A_prev, W, b, keep_prob, activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    WL, bL, keep_probL = parameters['W' + str(L)], parameters['b' + str(L)], parameters['keep_prob' + str(L)]\n",
    "    AL, cache = linear_activation_forward_dropout(A, WL, bL, keep_probL, activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "id": "50f636cdc178e556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cost function",
   "id": "721ae584741bcf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    :param AL: probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "    :param Y: true \"label\" vector\n",
    "    :return: cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ],
   "id": "b1ce9a75e8dcee78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t_Y = np.asarray([[1, 1, 0]])\n",
    "t_AL = np.array([[0.8, 0.9, 0.4]])\n",
    "t_cost = compute_cost(t_AL, t_Y)\n",
    "assert np.allclose(t_cost, 0.2797765635793422)"
   ],
   "id": "10f4fd829be9e49f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Backward propagation",
   "id": "10414c6d1d6d8a2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "9e99ffd08853a793"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ],
   "id": "d111bb470b32c5c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ],
   "id": "f683bb6d2bbd068e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ],
   "id": "327d27e19bd420c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_dZ = np.random.randn(3,4)\n",
    "t_linear_cache = (np.random.randn(5,4), np.random.randn(3,5), np.random.randn(3,1))\n",
    "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [-1.15171336, 0.06718465, -0.3204696, 2.09812712],\n",
    "    [ 0.60345879, -3.72508701, 5.81700741, -3.84326836],\n",
    "    [-0.4319552, -1.30987417, 1.72354705, 0.05070578],\n",
    "    [-0.38981415, 0.60811244, -1.25938424, 1.47191593],\n",
    "    [-2.52214926, 2.67882552, -0.67947465, 1.48119548],\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.07313866, -0.0976715, -0.87585828, 0.73763362, 0.00785716],\n",
    "    [0.85508818, 0.37530413, -0.59912655, 0.71278189, -0.58931808],\n",
    "    [0.97913304, -0.24376494, -0.08839671, 0.55151192, -0.10290907],\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.14713786],\n",
    "    [-0.11313155],\n",
    "    [-0.13209101]\n",
    "]))"
   ],
   "id": "94ce7c376473b2d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Backward (no dropout)",
   "id": "ada9cbaa5036fbd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward_dropout(dA, cache, activation):\n",
    "    linear_cache, activation_cache, _ = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ],
   "id": "dff6636099c9c902",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_dAL = np.random.randn(1,2)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z = np.random.randn(1,2)\n",
    "t_linear_activation_cache = ((t_A, t_W, t_b), t_Z)\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.11017994, 0.01105339],\n",
    "    [0.09466817, 0.00949723],\n",
    "    [-0.05743092, -0.00576154]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.10266786, 0.09778551, -0.01968084]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.05729622]\n",
    "]))\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.44090989, 0.0],\n",
    "    [0.37883606, 0.0],\n",
    "    [-0.2298228, 0.0]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.44513824, 0.37371418, -0.10478989]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.20837892]\n",
    "]))"
   ],
   "id": "f55b269302607faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def backward_propagate(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "    return grads"
   ],
   "id": "fecaa2dc24d5e936",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "t_AL = np.random.randn(1, 2)\n",
    "t_Y = np.array([[1, 0]])\n",
    "\n",
    "t_A1 = np.random.randn(4, 2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_Z1 = np.random.randn(3,2)\n",
    "t_linear_cache_activation1 = ((t_A1, t_W1, t_b1), t_Z1)\n",
    "\n",
    "t_A2 = np.random.randn(3,2)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_Z2 = np.random.randn(1,2)\n",
    "t_linear_cache_activation2 = ((t_A2, t_W2, t_b2), t_Z2)\n",
    "\n",
    "t_AL, t_Y_assess, t_caches = t_AL, t_Y, (t_linear_cache_activation1, t_linear_cache_activation2)\n",
    "t_grads = backward_propagate(t_AL, t_Y_assess, t_caches)\n",
    "\n",
    "assert np.allclose(t_grads['dA0'], np.array([\n",
    "    [0.0, 0.52257901],\n",
    "    [0.0, -0.3269206],\n",
    "    [0.0, -0.32070404],\n",
    "    [0.0, -0.74079187]\n",
    "]))\n",
    "assert np.allclose(t_grads['dA1'], np.array([\n",
    "    [0.12913162, -0.44014127],\n",
    "    [-0.14175655, 0.48317296],\n",
    "    [0.01663708, -0.05670698]\n",
    "]))\n",
    "assert np.allclose(t_grads['dW1'], np.array([\n",
    "    [0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.05283652, 0.01005865, 0.01777766, 0.0135308 ],\n",
    "]))\n",
    "assert np.allclose(t_grads['dW2'], np.array([\n",
    "    [-0.39202432, -0.13325855, -0.04601089]\n",
    "]))\n",
    "assert np.allclose(t_grads['db1'], np.array([\n",
    "    [-0.22007063],\n",
    "    [0.0],\n",
    "    [-0.02835349]\n",
    "]))\n",
    "assert np.allclose(t_grads['db2'], np.array([\n",
    "    [0.15187861]\n",
    "]))"
   ],
   "id": "d677715e675b67b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Backward (with dropout)",
   "id": "1e0eecefdecee41f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def backward_propagate_dropout(AL, Y, parameters, caches):\n",
    "    grads = {}\n",
    "    L = parameters[\"L\"] # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    keep_probL = parameters[\"keep_prob\" + str(L)]\n",
    "    if keep_probL < 1.0:\n",
    "        _, _, D = caches[L-1]\n",
    "        dAL = np.multiply(dAL, D)\n",
    "        dAL = dAL / keep_probL\n",
    "\n",
    "    curr_cache = caches[L-1]\n",
    "    dA_prev, dW, db = linear_activation_backward_dropout(dAL, curr_cache, activation = \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = (dA_prev, dW, db)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        curr_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward_dropout(grads[\"dA\" + str(l+1)], curr_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = (dA_prev, dW, db)\n",
    "\n",
    "    for l in reversed(range(1, L)):\n",
    "        keep_prob = parameters[\"keep_prob\" + str(l)]\n",
    "        if keep_prob < 1.0:\n",
    "            _, _, D = caches[l-1]\n",
    "            dAl = grads[\"dA\" + str(l)]\n",
    "            dAl = np.multiply(dAl, D)\n",
    "            dAl = dAl / keep_prob\n",
    "            grads[\"dA\" + str(l)] = dAl\n",
    "\n",
    "    return grads"
   ],
   "id": "df839358373dd610",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Update parameters",
   "id": "4cfc915e5aea7a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Update parameters (without dropout)",
   "id": "ff8136673234adac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_parameters(parameters, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = parameters[\"L\"]\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    for l in range(1, L+1):\n",
    "        v[\"dW\" + str(l)] = beta1*v[\"dW\" + str(l)] + (1-beta1)*grads['dW' + str(l)]\n",
    "        v[\"db\" + str(l)] = beta1*v[\"db\" + str(l)] + (1-beta1)*grads['db' + str(l)]\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1-np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1-np.power(beta1, t))\n",
    "        s[\"dW\" + str(l)] = beta2*s[\"dW\" + str(l)] + (1-beta2)*np.square(grads['dW' + str(l)])\n",
    "        s[\"db\" + str(l)] = beta2*s[\"db\" + str(l)] + (1-beta2)*np.square(grads['db' + str(l)])\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1-np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1-np.power(beta2, t))\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * (v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon))\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * (v_corrected[\"db\" + str(l)] / (np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon))\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ],
   "id": "d26002ab614d98b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_parameters = {\"W1\": t_W1,\n",
    "                \"b1\": t_b1,\n",
    "                \"W2\": t_W2,\n",
    "                \"b2\": t_b2}\n",
    "\n",
    "np.random.seed(3)\n",
    "t_dW1 = np.random.randn(3,4)\n",
    "t_db1 = np.random.randn(3,1)\n",
    "t_dW2 = np.random.randn(1,3)\n",
    "t_db2 = np.random.randn(1,1)\n",
    "t_grads = {\"dW1\": t_dW1,\n",
    "           \"db1\": t_db1,\n",
    "           \"dW2\": t_dW2,\n",
    "           \"db2\": t_db2}\n",
    "t_parameters = update_parameters(t_parameters, t_grads, 0.1)\n",
    "\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [-0.59562069, -0.09991781, -2.14584584, 1.82662008],\n",
    "    [-1.76569676, -0.80627147, 0.51115557, -1.18258802],\n",
    "    [-1.0535704, -0.86128581, 0.68284052, 2.20374577]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array([\n",
    "    [-0.04659241],\n",
    "    [-1.28888275],\n",
    "    [0.53405496]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.55569196, 0.0354055, 1.32964895]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([\n",
    "    [-0.84610769]\n",
    "]))"
   ],
   "id": "fd838eaa60909e51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Update parameters (with dropout)",
   "id": "b6e944017016fad1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_parameters_dropout(parameters, grads, learning_rate):\n",
    "    L = parameters[\"L\"]\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ],
   "id": "b173d8883177264b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "a0b255e0e4bb1326"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "from common import CV_DATA_DIR"
   ],
   "id": "85f1d7647f1c1549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"train_catvnoncat.h5\", \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"test_catvnoncat.h5\", \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ],
   "id": "b47814d4d1ea5ddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()",
   "id": "8b0fac50f4cdd9e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print (\"Number of training examples:\", train_x_orig.shape[0])\n",
    "print (\"Number of testing examples:\", test_x_orig.shape[0])\n",
    "print (\"Each image is of size: (\" + str(train_x_orig.shape[1]) + \", \" + str(train_x_orig.shape[1]) + \", 3)\")\n",
    "\n",
    "assert train_x_orig.shape == (209, 64, 64, 3)\n",
    "assert train_y.shape == (1, 209)\n",
    "assert test_x_orig.shape == (50, 64, 64, 3)\n",
    "assert test_y.shape == (1, 50)"
   ],
   "id": "a0357974981bd6fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print(\"train_x's shape: \" + str(train_x.shape))\n",
    "print(\"test_x's shape: \" + str(test_x.shape))"
   ],
   "id": "fb1cacb5af286809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train L-layer model",
   "id": "9dd9c9e0bbc54fd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "a4f504179470b6c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:27:25.888482Z",
     "start_time": "2025-06-20T11:27:25.798503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_mini_batches(X, Y, mini_batch_size=64, seed=1):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    # Shuffle (X,Y)\n",
    "    np.random.seed(seed)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ],
   "id": "c06d0e255b28a723",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initialize_adam(parameters):\n",
    "    \"\"\"\n",
    "    Initialize v and s\n",
    "        - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
    "        - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \"\"\"\n",
    "    L = parameters[\"L\"]\n",
    "    v = {}\n",
    "    s = {}\n",
    "    for l in range(1, L+1):\n",
    "        v[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
    "        v[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
    "        s[\"dW\" + str(l)] = np.zeros(parameters[\"W\" + str(l)].shape)\n",
    "        s[\"db\" + str(l)] = np.zeros(parameters[\"b\" + str(l)].shape)\n",
    "    return v, s"
   ],
   "id": "20559bc8fc16f54c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "    \"\"\"\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "\n",
    "    Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer\n",
    "    decay_rate -- Decay rate. Scalar\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar\n",
    "    \"\"\"\n",
    "    learning_rate = learning_rate0 / (1 + decay_rate * epoch_num)\n",
    "    return learning_rate"
   ],
   "id": "1fe847b7ed166796",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n",
    "    \"\"\"\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "\n",
    "    Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer.\n",
    "    decay_rate -- Decay rate. Scalar.\n",
    "    time_interval -- Number of epochs where you update the learning rate.\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar\n",
    "    \"\"\"\n",
    "    learning_rate = learning_rate0 / (1 + decay_rate * np.floor(epoch_num/time_interval))\n",
    "    return learning_rate"
   ],
   "id": "88296bfdfa9f2c39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train L-layer Model (without dropout)",
   "id": "9ef987cdffdad827"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:45:51.741221Z",
     "start_time": "2025-06-20T11:45:51.739500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.0007\n",
    "\n",
    "# NN layers configuration (input layer + hidden layers + output layer)\n",
    "layers_config = [\n",
    "\t{\"dims\": 12288, \"activation\": None},\n",
    "\t{\"dims\": 20, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 7, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 5, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "id": "5ec28d71255f185a",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:45:52.711135Z",
     "start_time": "2025-06-20T11:45:52.708651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def L_layer_model(X, Y, layers_config, learning_rate = 0.001, num_epochs = 3500, decay_rate=1, print_cost=False):\n",
    "    # Set start learning rate\n",
    "    learning_rate0 = learning_rate\n",
    "\n",
    "    # Initialize NN parameters\n",
    "    parameters = initialize_parameters(layers_config)\n",
    "    # Initialize the optimizer parameters\n",
    "    v, s = initialize_adam(parameters)\n",
    "\n",
    "    t = 0\n",
    "    seed = 10\n",
    "    costs = []\n",
    "\n",
    "    for i in range(0, num_epochs):\n",
    "        # Forward propagation\n",
    "        AL, caches = forward_propagate(X, parameters)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # Backward propagation\n",
    "        grads = backward_propagate(AL, Y, caches)\n",
    "        # Update parameters\n",
    "        t = t + 1\n",
    "        parameters, v, s, _, _ = update_parameters(parameters, grads, v, s, t, learning_rate)\n",
    "        # Update learning rate\n",
    "        learning_rate = schedule_lr_decay(learning_rate0, i, decay_rate)\n",
    "        # Stora cost values to plot\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        # Print cost value\n",
    "        if print_cost and (i % 100 == 0 or i == num_epochs - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "\n",
    "    return parameters, costs"
   ],
   "id": "2557736fb8cfb5c1",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:47:40.692207Z",
     "start_time": "2025-06-20T11:47:24.281422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters, costs = L_layer_model(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    layers_config,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=3000,\n",
    "    print_cost = True)"
   ],
   "id": "d1bb96e07147bb37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692679777968078\n",
      "Cost after iteration 100: 0.3513411718572191\n",
      "Cost after iteration 200: 0.25870642905979385\n",
      "Cost after iteration 300: 0.2281714745548866\n",
      "Cost after iteration 400: 0.17829990667323362\n",
      "Cost after iteration 500: 0.14218844909152487\n",
      "Cost after iteration 600: 0.1083592841070296\n",
      "Cost after iteration 700: 0.07604753678388523\n",
      "Cost after iteration 800: 0.04771361154576466\n",
      "Cost after iteration 900: 0.035018645506220795\n",
      "Cost after iteration 1000: 0.029982037809414313\n",
      "Cost after iteration 1100: 0.02855193693725117\n",
      "Cost after iteration 1200: 0.02755985262985629\n",
      "Cost after iteration 1300: 0.02687895967734289\n",
      "Cost after iteration 1400: 0.026394581320925623\n",
      "Cost after iteration 1500: 0.02606927214007504\n",
      "Cost after iteration 1600: 0.025840901668327414\n",
      "Cost after iteration 1700: 0.025666375437617017\n",
      "Cost after iteration 1800: 0.025562104175944206\n",
      "Cost after iteration 1900: 0.025490643439223436\n",
      "Cost after iteration 2000: 0.02543184756242884\n",
      "Cost after iteration 2100: 0.025409293709815876\n",
      "Cost after iteration 2200: 0.02538782358214687\n",
      "Cost after iteration 2300: 0.025376090783723566\n",
      "Cost after iteration 2400: 0.02535949165568735\n",
      "Cost after iteration 2500: 0.025354233289290946\n",
      "Cost after iteration 2600: 0.02534563569373704\n",
      "Cost after iteration 2700: 0.025340739564521514\n",
      "Cost after iteration 2800: 0.025334268864499486\n",
      "Cost after iteration 2900: 0.025328614652731552\n",
      "Cost after iteration 2999: 0.025319260345065955\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T11:47:42.829520Z",
     "start_time": "2025-06-20T11:47:42.787497Z"
    }
   },
   "cell_type": "code",
   "source": "plot_costs(costs, learning_rate)",
   "id": "f320a01b93f03f9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train L-layer Model (with dropout)",
   "id": "988e7967aef48e2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "layers_config_dropout = [\n",
    "\t{\"dims\": 12288, \"keep_prob\": 1.0, \"activation\": None},\n",
    "\t{\"dims\": 20, \"keep_prob\": 0.935, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 7, \"keep_prob\": 0.97, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 5, \"keep_prob\": 1.0, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 1, \"keep_prob\": 1.0, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "id": "3d6522f3cc982d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def L_layer_model_dropout(X, Y, layers, learning_rate = 0.0075, num_iterations = 3500, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters_dropout(layers)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward_propagate_dropout(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagate_dropout(AL, Y, parameters, caches)\n",
    "        parameters = update_parameters_dropout(parameters, grads, learning_rate)\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ],
   "id": "5e59dfdc08c6b530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parameters_dropout, costs_dropout = L_layer_model_dropout(\n",
    "    train_x, train_y,\n",
    "    layers_config_dropout,\n",
    "    num_iterations = 4500,\n",
    "    print_cost = True)"
   ],
   "id": "aa81b270d7de26d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_costs(costs_dropout, learning_rate)",
   "id": "f214f5bc8b52fde4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "f0f703bd86e86840"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction (without dropout)",
   "id": "215745db1dc064e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = forward_propagate(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    accuracy = np.sum((p == y) / m)\n",
    "    return p, accuracy"
   ],
   "id": "b8c62dcbe48c36c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, accuracy = predict(train_x, train_y, parameters)\n",
    "print(\"Accuracy (train):\", accuracy)\n",
    "_, accuracy = predict(test_x, test_y, parameters)\n",
    "print(\"Accuracy (test):\", accuracy)"
   ],
   "id": "4da2e9c768b9542c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction (with dropout)",
   "id": "a25962460149e6bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_dropout(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = forward_propagate_dropout(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    accuracy = np.sum((p == y) / m)\n",
    "    return p, accuracy"
   ],
   "id": "1fcb122c98491911",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, accuracy_dropout = predict_dropout(train_x, train_y, parameters_dropout)\n",
    "print(\"Accuracy (train):\", accuracy_dropout)\n",
    "_, accuracy_dropout = predict_dropout(test_x, test_y, parameters_dropout)\n",
    "print(\"Accuracy (test):\", accuracy_dropout)"
   ],
   "id": "857f963c1177a086",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Checking\n",
    "\n",
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and loss function.\n",
    "\n",
    "Definition of a derivative (or gradient):$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$$\n",
    "\n",
    "For each i in parameters:\n",
    "- Compute `J_p[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(param_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagate(X, vec_to_dic(`$\\theta^{+}$`, param_shapes))`.\n",
    "- To compute `J_m[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Where gradapprox[i] is an approximation of the gradient with respect to `param_values[i]`. Then comparing gradapprox vector with the gradients vector from backpropagation the difference can be received:\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n",
    "\n",
    "$10^{-7}$ or less is good, $10^{-5}$ need to double-check components, $10^{-3}$ - need to worry."
   ],
   "id": "17ec7d639912c1ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dic_to_vec(parameters):\n",
    "    theta = []\n",
    "    shapes = {}\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]:\n",
    "        shapes[key] = parameters[key].shape\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta, shapes"
   ],
   "id": "1d510081dcbae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vec_to_dic(param_values, param_shapes):\n",
    "    output = {}\n",
    "    offset = 0\n",
    "    for key,shape in param_shapes.items():\n",
    "        items = shape[0] * shape[1]\n",
    "        output[key] = param_values[offset:offset+items].reshape(shape)\n",
    "        offset += items\n",
    "    return output"
   ],
   "id": "c0a9985375c85326",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def grad_to_vec(gradients):\n",
    "    theta = []\n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\", \"dW4\", \"db4\"]:\n",
    "        new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count += 1\n",
    "    return theta"
   ],
   "id": "27bf93c91c41ec5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gradient_check(parameters, gradients, X, Y, N=100, epsilon=1e-7):\n",
    "    L = parameters[\"L\"]\n",
    "    param_values, param_shapes = dic_to_vec(parameters)\n",
    "    grads = grad_to_vec(gradients)\n",
    "\n",
    "    No = param_values.shape[0]\n",
    "    J_p = np.zeros((No, 1))\n",
    "    J_m = np.zeros((No, 1))\n",
    "    gradapproxs = np.zeros((No, 1))\n",
    "\n",
    "    assert N <= No, \\\n",
    "        \"N parameter is invalid\"\n",
    "\n",
    "    for i in range(N):\n",
    "        theta_p = np.copy(param_values)\n",
    "        theta_p[i] = theta_p[i] + epsilon\n",
    "        param_p = vec_to_dic(theta_p, param_shapes)\n",
    "        param_p[\"L\"] = L\n",
    "        Yp_hat, _ = forward_propagate(X, param_p)\n",
    "        J_p[i] = compute_cost(Yp_hat, Y)\n",
    "\n",
    "        theta_m = np.copy(param_values)\n",
    "        theta_m[i] = theta_m[i] - epsilon\n",
    "        param_m = vec_to_dic(theta_m, param_shapes)\n",
    "        param_m[\"L\"] = L\n",
    "        Ym_hat, _ = forward_propagate(X, param_m)\n",
    "        J_m[i] = compute_cost(Ym_hat, Y)\n",
    "\n",
    "        gradapproxs[i] = (J_p[i] - J_m[i]) / (2 * epsilon)\n",
    "\n",
    "    num = np.linalg.norm(grads[:N] - gradapproxs[:N])\n",
    "    den = np.linalg.norm(grads[:N]) + np.linalg.norm(gradapproxs[:N])\n",
    "    differences = num / den\n",
    "    return differences\n"
   ],
   "id": "3258e01cfaf240e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parameters = initialize_parameters(layers_config)\n",
    "AL, caches = forward_propagate(train_x, parameters)\n",
    "gradients = backward_propagate(AL, train_y, caches)\n",
    "\n",
    "difference = gradient_check(parameters, gradients, train_x, train_y)\n",
    "print(\"Difference:\", difference)"
   ],
   "id": "ad3bb3ae1f82e8bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
