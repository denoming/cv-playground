{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"images/deep_neural_network.png\" style=\"width:1024px;height:768px;\">",
   "id": "9e1679ef3969ec6e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.228173Z",
     "start_time": "2025-06-06T15:15:31.152720Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.random.seed(1)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize",
   "id": "387c868d79d2eeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.240498Z",
     "start_time": "2025-06-06T15:15:31.238160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Wl - weight matrix of shape (layer_dims[l], layer_dims[l-1])\\n\n",
    "    bl - bias vector of shape (layer_dims[l], 1)\n",
    "    :param layer_dims: array (list) containing the dimensions of each layer\n",
    "    :return: dictionary containing the parameters of each layer: \"W1\", \"b1\", ..., \"WL\", \"bL\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        # Initialize all weights using \"He\" initialization to eliminate vanishing/exploding gradients\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        # Initialize all biases to zero\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ],
   "id": "5decec3ffeb76cd2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.290512Z",
     "start_time": "2025-06-06T15:15:31.287303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_parameters = initialize_parameters([5,4,3])\n",
    "print(str(t_parameters[\"W1\"]))\n",
    "print(str(t_parameters[\"b1\"]))\n",
    "print(str(t_parameters[\"W2\"]))\n",
    "print(str(t_parameters[\"b2\"]))\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [1.02732621, -0.38690873, -0.33404515, -0.67860494, 0.54733184],\n",
    "    [-1.45562088, 1.10351585, -0.48142952, 0.20177804,-0.15771567],\n",
    "    [0.92471825, -1.30294739, -0.20391454, -0.2428973, 0.71705876],\n",
    "    [-0.69563232, -0.10905317, -0.55520641, 0.02669832, 0.36860471]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array(\n",
    "    [0, 0, 0, 0]\n",
    "))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.77825528, 0.8094419, 0.63752091, 0.35531715],\n",
    "    [ 0.63700135, -0.48346861, -0.08689651, -0.66168891],\n",
    "    [-0.18942548, 0.37501795, -0.48907801, -0.28054711]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([[0.]]))"
   ],
   "id": "aaefec3a45fb09e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.02732621 -0.38690873 -0.33404515 -0.67860494  0.54733184]\n",
      " [-1.45562088  1.10351585 -0.48142952  0.20177804 -0.15771567]\n",
      " [ 0.92471825 -1.30294739 -0.20391454 -0.2428973   0.71705876]\n",
      " [-0.69563232 -0.10905317 -0.55520641  0.02669832  0.36860471]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[-0.77825528  0.8094419   0.63752091  0.35531715]\n",
      " [ 0.63700135 -0.48346861 -0.08689651 -0.66168891]\n",
      " [-0.18942548  0.37501795 -0.48907801 -0.28054711]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Forward propagation",
   "id": "91c3fdd1722d0279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.339177Z",
     "start_time": "2025-06-06T15:15:31.337680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation\n",
    "    :param Z: numpy array of any shape\n",
    "    :return: output of sigmoid(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "b5db74f340bce00a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.443681Z",
     "start_time": "2025-06-06T15:15:31.441694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function\n",
    "    :param Z: Output of the linear layer, of any shape\n",
    "    :return:  output of relu(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "a41ba41f7c9ef5c8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.587336Z",
     "start_time": "2025-06-06T15:15:31.585508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "    :param A: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :return:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter\n",
    "        cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ],
   "id": "ea00b1e6b2eb28d6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.658846Z",
     "start_time": "2025-06-06T15:15:31.656895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
    "assert np.allclose(t_Z, np.array([3.26295337, -1.23429987]))"
   ],
   "id": "43c1e95e7a1842d6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.710264Z",
     "start_time": "2025-06-06T15:15:31.708420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :param activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    :return:\n",
    "        A -- the output of the activation function, also called the post-activation value\n",
    "        cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ],
   "id": "d3050cf481e6c9ca",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.781660Z",
     "start_time": "2025-06-06T15:15:31.779855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A_prev = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n",
    "assert np.allclose(t_A, np.array([[0.96313579, 0.22542973]]));\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n",
    "assert np.allclose(t_A, np.array([[3.26295337, 0.0]]));"
   ],
   "id": "a443fec0626799d8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.827036Z",
     "start_time": "2025-06-06T15:15:31.825102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_propagate(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # Number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR->RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "id": "50f636cdc178e556",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.876495Z",
     "start_time": "2025-06-06T15:15:31.874372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_X = np.random.randn(5,4)\n",
    "t_parameters = {\n",
    "    \"W1\": np.random.randn(4,5),\n",
    "    \"b1\": np.random.randn(4,1),\n",
    "    \"W2\": np.random.randn(3,4),\n",
    "    \"b2\": np.random.randn(3,1),\n",
    "    \"W3\": np.random.randn(1,3),\n",
    "    \"b3\": np.random.randn(1,1)\n",
    "}\n",
    "t_AL, t_caches = forward_propagate(t_X, t_parameters)\n",
    "print(\"AL = \" + str(t_AL))\n",
    "assert np.allclose(t_AL, np.array([[0.77634609, 0.9998399, 0.99021857, 0.33755508]]))"
   ],
   "id": "aea37651da749aa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.77634609 0.9998399  0.99021857 0.33755508]]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cost function",
   "id": "721ae584741bcf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:31.942537Z",
     "start_time": "2025-06-06T15:15:31.940667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    :param AL: probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "    :param Y: true \"label\" vector\n",
    "    :return: cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ],
   "id": "b1ce9a75e8dcee78",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.005047Z",
     "start_time": "2025-06-06T15:15:32.003499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t_Y = np.asarray([[1, 1, 0]])\n",
    "t_AL = np.array([[0.8, 0.9, 0.4]])\n",
    "t_cost = compute_cost(t_AL, t_Y)\n",
    "assert np.allclose(t_cost, 0.2797765635793422)"
   ],
   "id": "10f4fd829be9e49f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Backward propagation",
   "id": "10414c6d1d6d8a2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.054215Z",
     "start_time": "2025-06-06T15:15:32.052621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ],
   "id": "d111bb470b32c5c3",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.118781Z",
     "start_time": "2025-06-06T15:15:32.117363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ],
   "id": "f683bb6d2bbd068e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.189657Z",
     "start_time": "2025-06-06T15:15:32.188072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ],
   "id": "327d27e19bd420c1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.237804Z",
     "start_time": "2025-06-06T15:15:32.235623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_dZ = np.random.randn(3,4)\n",
    "t_linear_cache = (np.random.randn(5,4), np.random.randn(3,5), np.random.randn(3,1))\n",
    "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [-1.15171336, 0.06718465, -0.3204696, 2.09812712],\n",
    "    [ 0.60345879, -3.72508701, 5.81700741, -3.84326836],\n",
    "    [-0.4319552, -1.30987417, 1.72354705, 0.05070578],\n",
    "    [-0.38981415, 0.60811244, -1.25938424, 1.47191593],\n",
    "    [-2.52214926, 2.67882552, -0.67947465, 1.48119548],\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.07313866, -0.0976715, -0.87585828, 0.73763362, 0.00785716],\n",
    "    [0.85508818, 0.37530413, -0.59912655, 0.71278189, -0.58931808],\n",
    "    [0.97913304, -0.24376494, -0.08839671, 0.55151192, -0.10290907],\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.14713786],\n",
    "    [-0.11313155],\n",
    "    [-0.13209101]\n",
    "]))"
   ],
   "id": "94ce7c376473b2d4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.285856Z",
     "start_time": "2025-06-06T15:15:32.284123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ],
   "id": "dff6636099c9c902",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.333484Z",
     "start_time": "2025-06-06T15:15:32.331088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_dAL = np.random.randn(1,2)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z = np.random.randn(1,2)\n",
    "t_linear_activation_cache = ((t_A, t_W, t_b), t_Z)\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.11017994, 0.01105339],\n",
    "    [0.09466817, 0.00949723],\n",
    "    [-0.05743092, -0.00576154]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.10266786, 0.09778551, -0.01968084]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.05729622]\n",
    "]))\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.44090989, 0.0],\n",
    "    [0.37883606, 0.0],\n",
    "    [-0.2298228, 0.0]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.44513824, 0.37371418, -0.10478989]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.20837892]\n",
    "]))"
   ],
   "id": "f55b269302607faf",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.381450Z",
     "start_time": "2025-06-06T15:15:32.379367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_propagate(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ],
   "id": "fecaa2dc24d5e936",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.434033Z",
     "start_time": "2025-06-06T15:15:32.430841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "t_AL = np.random.randn(1, 2)\n",
    "t_Y = np.array([[1, 0]])\n",
    "\n",
    "t_A1 = np.random.randn(4, 2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_Z1 = np.random.randn(3,2)\n",
    "t_linear_cache_activation1 = ((t_A1, t_W1, t_b1), t_Z1)\n",
    "\n",
    "t_A2 = np.random.randn(3,2)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_Z2 = np.random.randn(1,2)\n",
    "t_linear_cache_activation2 = ((t_A2, t_W2, t_b2), t_Z2)\n",
    "\n",
    "t_AL, t_Y_assess, t_caches = t_AL, t_Y, (t_linear_cache_activation1, t_linear_cache_activation2)\n",
    "t_grads = backward_propagate(t_AL, t_Y_assess, t_caches)\n",
    "\n",
    "assert np.allclose(t_grads['dA0'], np.array([\n",
    "    [0.0, 0.52257901],\n",
    "    [0.0, -0.3269206],\n",
    "    [0.0, -0.32070404],\n",
    "    [0.0, -0.74079187]\n",
    "]))\n",
    "assert np.allclose(t_grads['dA1'], np.array([\n",
    "    [0.12913162, -0.44014127],\n",
    "    [-0.14175655, 0.48317296],\n",
    "    [0.01663708, -0.05670698]\n",
    "]))\n",
    "assert np.allclose(t_grads['dW1'], np.array([\n",
    "    [0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.05283652, 0.01005865, 0.01777766, 0.0135308 ],\n",
    "]))\n",
    "assert np.allclose(t_grads['dW2'], np.array([\n",
    "    [-0.39202432, -0.13325855, -0.04601089]\n",
    "]))\n",
    "assert np.allclose(t_grads['db1'], np.array([\n",
    "    [-0.22007063],\n",
    "    [0.0],\n",
    "    [-0.02835349]\n",
    "]))\n",
    "assert np.allclose(t_grads['db2'], np.array([\n",
    "    [0.15187861]\n",
    "]))"
   ],
   "id": "d677715e675b67b9",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Update parameters",
   "id": "4cfc915e5aea7a75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.495165Z",
     "start_time": "2025-06-06T15:15:32.493277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ],
   "id": "d26002ab614d98b1",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.545250Z",
     "start_time": "2025-06-06T15:15:32.542681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_parameters = {\"W1\": t_W1,\n",
    "                \"b1\": t_b1,\n",
    "                \"W2\": t_W2,\n",
    "                \"b2\": t_b2}\n",
    "\n",
    "np.random.seed(3)\n",
    "t_dW1 = np.random.randn(3,4)\n",
    "t_db1 = np.random.randn(3,1)\n",
    "t_dW2 = np.random.randn(1,3)\n",
    "t_db2 = np.random.randn(1,1)\n",
    "t_grads = {\"dW1\": t_dW1,\n",
    "           \"db1\": t_db1,\n",
    "           \"dW2\": t_dW2,\n",
    "           \"db2\": t_db2}\n",
    "t_parameters = update_parameters(t_parameters, t_grads, 0.1)\n",
    "\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [-0.59562069, -0.09991781, -2.14584584, 1.82662008],\n",
    "    [-1.76569676, -0.80627147, 0.51115557, -1.18258802],\n",
    "    [-1.0535704, -0.86128581, 0.68284052, 2.20374577]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array([\n",
    "    [-0.04659241],\n",
    "    [-1.28888275],\n",
    "    [0.53405496]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.55569196, 0.0354055, 1.32964895]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([\n",
    "    [-0.84610769]\n",
    "]))"
   ],
   "id": "fd838eaa60909e51",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train L-layer model",
   "id": "9dd9c9e0bbc54fd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.592769Z",
     "start_time": "2025-06-06T15:15:32.590884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward_propagate(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagate(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ],
   "id": "2557736fb8cfb5c1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "a0b255e0e4bb1326"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.853365Z",
     "start_time": "2025-06-06T15:15:32.641045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "from common import CV_DATA_DIR"
   ],
   "id": "85f1d7647f1c1549",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.862382Z",
     "start_time": "2025-06-06T15:15:32.860531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"train_catvnoncat.h5\", \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"test_catvnoncat.h5\", \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ],
   "id": "b47814d4d1ea5ddc",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.961705Z",
     "start_time": "2025-06-06T15:15:32.913300Z"
    }
   },
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()",
   "id": "8b0fac50f4cdd9e9",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:32.991114Z",
     "start_time": "2025-06-06T15:15:32.989171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print (\"Number of training examples:\", train_x_orig.shape[0])\n",
    "print (\"Number of testing examples:\", test_x_orig.shape[0])\n",
    "print (\"Each image is of size: (\" + str(train_x_orig.shape[1]) + \", \" + str(train_x_orig.shape[1]) + \", 3)\")\n",
    "\n",
    "assert train_x_orig.shape == (209, 64, 64, 3)\n",
    "assert train_y.shape == (1, 209)\n",
    "assert test_x_orig.shape == (50, 64, 64, 3)\n",
    "assert test_y.shape == (1, 50)"
   ],
   "id": "a0357974981bd6fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:33.047170Z",
     "start_time": "2025-06-06T15:15:33.042157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ],
   "id": "fb1cacb5af286809",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "62d58197c30e8583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:33.107517Z",
     "start_time": "2025-06-06T15:15:33.106150Z"
    }
   },
   "cell_type": "code",
   "source": "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model",
   "id": "dc7c8bd4a0858832",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:33.178175Z",
     "start_time": "2025-06-06T15:15:33.157787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1, print_cost = False)\n",
    "print(\"Cost after first iteration: \" + str(costs[0]))"
   ],
   "id": "42284dd411871222",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after first iteration: 1.208125010600323\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.193589Z",
     "start_time": "2025-06-06T15:15:33.246101Z"
    }
   },
   "cell_type": "code",
   "source": "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)",
   "id": "7833f471c166879b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.208125010600323\n",
      "Cost after iteration 100: 0.6740577413973853\n",
      "Cost after iteration 200: 0.6625049864829947\n",
      "Cost after iteration 300: 0.655233447204667\n",
      "Cost after iteration 400: 0.6504891112622437\n",
      "Cost after iteration 500: 0.6473194475269372\n",
      "Cost after iteration 600: 0.6449422527912865\n",
      "Cost after iteration 700: 0.6430979384805492\n",
      "Cost after iteration 800: 0.6414454339232503\n",
      "Cost after iteration 900: 0.6398930793986607\n",
      "Cost after iteration 1000: 0.6374058448197395\n",
      "Cost after iteration 1100: 0.6353225187839247\n",
      "Cost after iteration 1200: 0.6334206995910424\n",
      "Cost after iteration 1300: 0.6309078606523231\n",
      "Cost after iteration 1400: 0.6278703533430822\n",
      "Cost after iteration 1500: 0.6243486119913267\n",
      "Cost after iteration 1600: 0.6206564914287053\n",
      "Cost after iteration 1700: 0.6165605693183634\n",
      "Cost after iteration 1800: 0.5949506456607018\n",
      "Cost after iteration 1900: 0.5707369311105821\n",
      "Cost after iteration 2000: 0.5391408405511141\n",
      "Cost after iteration 2100: 0.4834937236199692\n",
      "Cost after iteration 2200: 0.5131169900573237\n",
      "Cost after iteration 2300: 0.3883914293840326\n",
      "Cost after iteration 2400: 0.505813928809461\n",
      "Cost after iteration 2499: 0.31447657338494434\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "f0f703bd86e86840"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.226508Z",
     "start_time": "2025-06-06T15:15:46.224316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = forward_propagate(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    accuracy = np.sum((p == y) / m)\n",
    "    return p, accuracy"
   ],
   "id": "b8c62dcbe48c36c5",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.277017Z",
     "start_time": "2025-06-06T15:15:46.273351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred_train, accuracy = predict(train_x, train_y, parameters)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "assert np.greater(accuracy, 0.9)"
   ],
   "id": "5c700cc866846127",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9186602870813396\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Checking\n",
    "\n",
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and loss function.\n",
    "\n",
    "Definition of a derivative (or gradient):$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "For each i in parameters:\n",
    "- Compute `J_p[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(param_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagate(X, vec_to_dic(`$\\theta^{+}$`, param_shapes))`.\n",
    "- To compute `J_m[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Where gradapprox[i] is an approximation of the gradient with respect to `param_values[i]`. Then comparing gradapprox vector with the gradients vector from backpropagation the difference can be received:\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n",
    "\n",
    "$10^{-7}$ or less is good, $10^{-5}$ need to double-check components, $10^{-3}$ - need to worry."
   ],
   "id": "17ec7d639912c1ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.364816Z",
     "start_time": "2025-06-06T15:15:46.363051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dic_to_vec(parameters):\n",
    "    theta = []\n",
    "    shapes = {}\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]:\n",
    "        shapes[key] = parameters[key].shape\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta, shapes"
   ],
   "id": "1d510081dcbae4",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.432595Z",
     "start_time": "2025-06-06T15:15:46.431062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vec_to_dic(param_values, param_shapes):\n",
    "    parameters = {}\n",
    "    offset = 0\n",
    "    for key,shape in param_shapes.items():\n",
    "        items = shape[0] * shape[1]\n",
    "        parameters[key] = param_values[offset:offset+items].reshape(shape)\n",
    "        offset += items\n",
    "    return parameters"
   ],
   "id": "c0a9985375c85326",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.498634Z",
     "start_time": "2025-06-06T15:15:46.496921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def grad_to_vec(gradients):\n",
    "    theta = []\n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\", \"dW4\", \"db4\"]:\n",
    "        new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count += 1\n",
    "    return theta"
   ],
   "id": "27bf93c91c41ec5d",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.559837Z",
     "start_time": "2025-06-06T15:15:46.557480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_check(parameters, gradients, X, Y, N=100, epsilon=1e-7):\n",
    "    param_values, param_shapes = dic_to_vec(parameters)\n",
    "    grads = grad_to_vec(gradients)\n",
    "\n",
    "    No = param_values.shape[0]\n",
    "    J_p = np.zeros((No, 1))\n",
    "    J_m = np.zeros((No, 1))\n",
    "    gradapproxs = np.zeros((No, 1))\n",
    "\n",
    "    assert N <= No, \\\n",
    "        \"N parameter is invalid\"\n",
    "\n",
    "    for i in range(N):\n",
    "        theta_p = np.copy(param_values)\n",
    "        theta_p[i] = theta_p[i] + epsilon\n",
    "        Yp_hat, _ = forward_propagate(X, vec_to_dic(theta_p, param_shapes))\n",
    "        J_p[i] = compute_cost(Yp_hat, Y)\n",
    "\n",
    "        theta_m = np.copy(param_values)\n",
    "        theta_m[i] = theta_m[i] - epsilon\n",
    "        Ym_hat, _ = forward_propagate(X, vec_to_dic(theta_m, param_shapes))\n",
    "        J_m[i] = compute_cost(Ym_hat, Y)\n",
    "\n",
    "        gradapproxs[i] = (J_p[i] - J_m[i]) / (2 * epsilon)\n",
    "\n",
    "    num = np.linalg.norm(grads[:N] - gradapproxs[:N])\n",
    "    den = np.linalg.norm(grads[:N]) + np.linalg.norm(gradapproxs[:N])\n",
    "    differences = num / den\n",
    "    return differences\n"
   ],
   "id": "3258e01cfaf240e2",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T15:15:46.782100Z",
     "start_time": "2025-06-06T15:15:46.607876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameters = initialize_parameters(layers_dims)\n",
    "AL, caches = forward_propagate(train_x, parameters)\n",
    "gradients = backward_propagate(AL, train_y, caches)\n",
    "\n",
    "difference = gradient_check(parameters, gradients, train_x, train_y)\n",
    "print(\"Difference:\", difference)"
   ],
   "id": "ad3bb3ae1f82e8bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: 1.5296167016981527e-07\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
