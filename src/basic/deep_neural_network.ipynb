{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"images/deep_neural_network.png\" style=\"width:1024px;height:768px;\">",
   "id": "9e1679ef3969ec6e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ],
   "id": "c06b333a6035f70c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialize",
   "id": "387c868d79d2eeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Without Dropout",
   "id": "2174a2e3f776acf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initialize_parameters(layers):\n",
    "    \"\"\"\n",
    "    Wl - weight matrix of shape (layer_dims[l], layer_dims[l-1])\\n\n",
    "    bl - bias vector of shape (layer_dims[l], 1)\n",
    "    :param layer_dims: array (list) containing the dimensions of each layer\n",
    "    :return: dictionary containing the parameters of each layer: \"W1\", \"b1\", ..., \"WL\", \"bL\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layers)\n",
    "    for l in range(1, L):\n",
    "        curr, prev = layers[l], layers[l-1]\n",
    "        dimsP = prev[\"dims\"]\n",
    "        dimsC, activC = curr[\"dims\"], curr[\"activation\"]\n",
    "        # Initialize all weights using \"He\"/\"Xavier\" initialization to eliminate vanishing/exploding gradients\n",
    "        parameters['W' + str(l)] = np.random.randn(dimsC, dimsP)\n",
    "        if activC == \"relu\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(2 / dimsP)\n",
    "        elif activC == \"sigmoid\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(1 / dimsP)\n",
    "        else:\n",
    "            parameters['W' + str(l)] *= 0.01\n",
    "        # Initialize all biases to zero\n",
    "        parameters['b' + str(l)] = np.zeros((dimsC, 1))\n",
    "    return parameters"
   ],
   "id": "5decec3ffeb76cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_parameters = initialize_parameters([\n",
    "\t{\"dims\": 5, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 4, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 3, \"activation\": \"relu\"},\n",
    "])\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [1.02732621, -0.38690873, -0.33404515, -0.67860494, 0.54733184],\n",
    "    [-1.45562088, 1.10351585, -0.48142952, 0.20177804,-0.15771567],\n",
    "    [0.92471825, -1.30294739, -0.20391454, -0.2428973, 0.71705876],\n",
    "    [-0.69563232, -0.10905317, -0.55520641, 0.02669832, 0.36860471]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array(\n",
    "    [0, 0, 0, 0]\n",
    "))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.77825528, 0.8094419, 0.63752091, 0.35531715],\n",
    "    [ 0.63700135, -0.48346861, -0.08689651, -0.66168891],\n",
    "    [-0.18942548, 0.37501795, -0.48907801, -0.28054711]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([[0.]]))"
   ],
   "id": "460e7d313876909e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## With Dropout",
   "id": "401e3180155cc4fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initialize_parameters_dropout(layers):\n",
    "    parameters = {}\n",
    "    L = len(layers)\n",
    "    for l in range(1, L):\n",
    "        curr, prev = layers[l], layers[l-1]\n",
    "        dimsP = prev[\"dims\"]\n",
    "        dimsC, keep_probsC, activC = curr[\"dims\"], curr[\"keep_prob\"], curr[\"activation\"]\n",
    "        parameters['W' + str(l)] = np.random.randn(dimsC, dimsP)\n",
    "        if activC == \"relu\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(2 / dimsP)\n",
    "        elif activC == \"sigmoid\":\n",
    "            parameters['W' + str(l)] *= np.sqrt(1 / dimsP)\n",
    "        else:\n",
    "            parameters['W' + str(l)] *= 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((dimsC, 1))\n",
    "        parameters['keep_prob' + str(l)] = keep_probsC\n",
    "    return parameters"
   ],
   "id": "c920cbf74bd7962",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Forward propagation",
   "id": "91c3fdd1722d0279"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "ab4e8611d1b4b61c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation\n",
    "    :param Z: numpy array of any shape\n",
    "    :return: output of sigmoid(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "b5db74f340bce00a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function\n",
    "    :param Z: Output of the linear layer, of any shape\n",
    "    :return:  output of relu(z), same shape as Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "id": "a41ba41f7c9ef5c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "    :param A: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :return:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter\n",
    "        cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ],
   "id": "d034fedf9e3fa68e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
    "assert np.allclose(t_Z, np.array([3.26295337, -1.23429987]))"
   ],
   "id": "b79103a0e69e3f92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Forward (without dropout)",
   "id": "7ad1580d4c488c49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    :param A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    :param W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    :param activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    :return:\n",
    "        A -- the output of the activation function, also called the post-activation value\n",
    "        cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "    \"\"\"\n",
    "    A, linear_cache, activation_cache = None, None, None\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ],
   "id": "d3050cf481e6c9ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_A_prev = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n",
    "assert np.allclose(t_A, np.array([[0.96313579, 0.22542973]]));\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n",
    "assert np.allclose(t_A, np.array([[3.26295337, 0.0]]));"
   ],
   "id": "a443fec0626799d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_propagate(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # Number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR->RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "id": "8484174c6a412fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_X = np.random.randn(5,4)\n",
    "t_parameters = {\n",
    "    \"W1\": np.random.randn(4,5),\n",
    "    \"b1\": np.random.randn(4,1),\n",
    "    \"W2\": np.random.randn(3,4),\n",
    "    \"b2\": np.random.randn(3,1),\n",
    "    \"W3\": np.random.randn(1,3),\n",
    "    \"b3\": np.random.randn(1,1)\n",
    "}\n",
    "t_AL, t_caches = forward_propagate(t_X, t_parameters)\n",
    "print(\"AL = \" + str(t_AL))\n",
    "assert np.allclose(t_AL, np.array([[0.77634609, 0.9998399, 0.99021857, 0.33755508]]))"
   ],
   "id": "a23e169e9445e216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Forward (with dropout)",
   "id": "ecebe8fd6aae9f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_activation_forward_dropout(A_prev, W, b, keep_prob, activation):\n",
    "    A, linear_cache, activation_cache = None, None, None\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    D = None\n",
    "    if keep_prob < 1.0:\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_prob).astype(int)\n",
    "        A = np.multiply(A, D)\n",
    "        A = A / keep_prob\n",
    "\n",
    "    cache = (linear_cache, activation_cache, D)\n",
    "    return A, cache"
   ],
   "id": "a5d60902cb51f400",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_propagate_dropout(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 3 # Number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR->RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W, b, keep_prob = parameters['W' + str(l)], parameters['b' + str(l)], parameters['keep_prob' + str(l)]\n",
    "        A, cache = linear_activation_forward_dropout(A_prev, W, b, keep_prob, activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    WL, bL, keep_probL = parameters['W' + str(L)], parameters['b' + str(L)], parameters['keep_prob' + str(L)]\n",
    "    AL, cache = linear_activation_forward_dropout(A, WL, bL, keep_probL, activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "id": "50f636cdc178e556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cost function",
   "id": "721ae584741bcf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    :param AL: probability vector corresponding to label predictions, shape (1, number of examples)\n",
    "    :param Y: true \"label\" vector\n",
    "    :return: cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ],
   "id": "b1ce9a75e8dcee78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t_Y = np.asarray([[1, 1, 0]])\n",
    "t_AL = np.array([[0.8, 0.9, 0.4]])\n",
    "t_cost = compute_cost(t_AL, t_Y)\n",
    "assert np.allclose(t_cost, 0.2797765635793422)"
   ],
   "id": "10f4fd829be9e49f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Backward propagation",
   "id": "10414c6d1d6d8a2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common",
   "id": "9e99ffd08853a793"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ],
   "id": "d111bb470b32c5c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ],
   "id": "f683bb6d2bbd068e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ],
   "id": "327d27e19bd420c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "t_dZ = np.random.randn(3,4)\n",
    "t_linear_cache = (np.random.randn(5,4), np.random.randn(3,5), np.random.randn(3,1))\n",
    "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [-1.15171336, 0.06718465, -0.3204696, 2.09812712],\n",
    "    [ 0.60345879, -3.72508701, 5.81700741, -3.84326836],\n",
    "    [-0.4319552, -1.30987417, 1.72354705, 0.05070578],\n",
    "    [-0.38981415, 0.60811244, -1.25938424, 1.47191593],\n",
    "    [-2.52214926, 2.67882552, -0.67947465, 1.48119548],\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.07313866, -0.0976715, -0.87585828, 0.73763362, 0.00785716],\n",
    "    [0.85508818, 0.37530413, -0.59912655, 0.71278189, -0.58931808],\n",
    "    [0.97913304, -0.24376494, -0.08839671, 0.55151192, -0.10290907],\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.14713786],\n",
    "    [-0.11313155],\n",
    "    [-0.13209101]\n",
    "]))"
   ],
   "id": "94ce7c376473b2d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Backward (no dropout)",
   "id": "ada9cbaa5036fbd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward_dropout(dA, cache, activation):\n",
    "    linear_cache, activation_cache, _ = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ],
   "id": "dff6636099c9c902",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_dAL = np.random.randn(1,2)\n",
    "t_A = np.random.randn(3,2)\n",
    "t_W = np.random.randn(1,3)\n",
    "t_b = np.random.randn(1,1)\n",
    "t_Z = np.random.randn(1,2)\n",
    "t_linear_activation_cache = ((t_A, t_W, t_b), t_Z)\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.11017994, 0.01105339],\n",
    "    [0.09466817, 0.00949723],\n",
    "    [-0.05743092, -0.00576154]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.10266786, 0.09778551, -0.01968084]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.05729622]\n",
    "]))\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n",
    "assert np.allclose(t_dA_prev, np.array([\n",
    "    [0.44090989, 0.0],\n",
    "    [0.37883606, 0.0],\n",
    "    [-0.2298228, 0.0]\n",
    "]))\n",
    "assert np.allclose(t_dW, np.array([\n",
    "    [0.44513824, 0.37371418, -0.10478989]\n",
    "]))\n",
    "assert np.allclose(t_db, np.array([\n",
    "    [-0.20837892]\n",
    "]))"
   ],
   "id": "f55b269302607faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def backward_propagate(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "    return grads"
   ],
   "id": "fecaa2dc24d5e936",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "t_AL = np.random.randn(1, 2)\n",
    "t_Y = np.array([[1, 0]])\n",
    "\n",
    "t_A1 = np.random.randn(4, 2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_Z1 = np.random.randn(3,2)\n",
    "t_linear_cache_activation1 = ((t_A1, t_W1, t_b1), t_Z1)\n",
    "\n",
    "t_A2 = np.random.randn(3,2)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_Z2 = np.random.randn(1,2)\n",
    "t_linear_cache_activation2 = ((t_A2, t_W2, t_b2), t_Z2)\n",
    "\n",
    "t_AL, t_Y_assess, t_caches = t_AL, t_Y, (t_linear_cache_activation1, t_linear_cache_activation2)\n",
    "t_grads = backward_propagate(t_AL, t_Y_assess, t_caches)\n",
    "\n",
    "assert np.allclose(t_grads['dA0'], np.array([\n",
    "    [0.0, 0.52257901],\n",
    "    [0.0, -0.3269206],\n",
    "    [0.0, -0.32070404],\n",
    "    [0.0, -0.74079187]\n",
    "]))\n",
    "assert np.allclose(t_grads['dA1'], np.array([\n",
    "    [0.12913162, -0.44014127],\n",
    "    [-0.14175655, 0.48317296],\n",
    "    [0.01663708, -0.05670698]\n",
    "]))\n",
    "assert np.allclose(t_grads['dW1'], np.array([\n",
    "    [0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.05283652, 0.01005865, 0.01777766, 0.0135308 ],\n",
    "]))\n",
    "assert np.allclose(t_grads['dW2'], np.array([\n",
    "    [-0.39202432, -0.13325855, -0.04601089]\n",
    "]))\n",
    "assert np.allclose(t_grads['db1'], np.array([\n",
    "    [-0.22007063],\n",
    "    [0.0],\n",
    "    [-0.02835349]\n",
    "]))\n",
    "assert np.allclose(t_grads['db2'], np.array([\n",
    "    [0.15187861]\n",
    "]))"
   ],
   "id": "d677715e675b67b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Backward (with dropout)",
   "id": "1e0eecefdecee41f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def backward_propagate_dropout(AL, Y, parameters, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    keep_probL = parameters[\"keep_prob\" + str(L)]\n",
    "    if keep_probL < 1.0:\n",
    "        _, _, D = caches[L-1]\n",
    "        dAL = np.multiply(dAL, D)\n",
    "        dAL = dAL / keep_probL\n",
    "\n",
    "    curr_cache = caches[L-1]\n",
    "    dA_prev, dW, db = linear_activation_backward_dropout(dAL, curr_cache, activation = \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = (dA_prev, dW, db)\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        curr_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward_dropout(grads[\"dA\" + str(l+1)], curr_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = (dA_prev, dW, db)\n",
    "\n",
    "    for l in reversed(range(1, L)):\n",
    "        keep_prob = parameters[\"keep_prob\" + str(l)]\n",
    "        if keep_prob < 1.0:\n",
    "            _, _, D = caches[l-1]\n",
    "            dAl = grads[\"dA\" + str(l)]\n",
    "            dAl = np.multiply(dAl, D)\n",
    "            dAl = dAl / keep_prob\n",
    "            grads[\"dA\" + str(l)] = dAl\n",
    "\n",
    "    return grads"
   ],
   "id": "df839358373dd610",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Update parameters",
   "id": "4cfc915e5aea7a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Update parameters (without dropout)",
   "id": "ff8136673234adac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ],
   "id": "d26002ab614d98b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(2)\n",
    "t_W1 = np.random.randn(3,4)\n",
    "t_b1 = np.random.randn(3,1)\n",
    "t_W2 = np.random.randn(1,3)\n",
    "t_b2 = np.random.randn(1,1)\n",
    "t_parameters = {\"W1\": t_W1,\n",
    "                \"b1\": t_b1,\n",
    "                \"W2\": t_W2,\n",
    "                \"b2\": t_b2}\n",
    "\n",
    "np.random.seed(3)\n",
    "t_dW1 = np.random.randn(3,4)\n",
    "t_db1 = np.random.randn(3,1)\n",
    "t_dW2 = np.random.randn(1,3)\n",
    "t_db2 = np.random.randn(1,1)\n",
    "t_grads = {\"dW1\": t_dW1,\n",
    "           \"db1\": t_db1,\n",
    "           \"dW2\": t_dW2,\n",
    "           \"db2\": t_db2}\n",
    "t_parameters = update_parameters(t_parameters, t_grads, 0.1)\n",
    "\n",
    "assert np.allclose(t_parameters[\"W1\"], np.array([\n",
    "    [-0.59562069, -0.09991781, -2.14584584, 1.82662008],\n",
    "    [-1.76569676, -0.80627147, 0.51115557, -1.18258802],\n",
    "    [-1.0535704, -0.86128581, 0.68284052, 2.20374577]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b1\"], np.array([\n",
    "    [-0.04659241],\n",
    "    [-1.28888275],\n",
    "    [0.53405496]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"W2\"], np.array([\n",
    "    [-0.55569196, 0.0354055, 1.32964895]\n",
    "]))\n",
    "assert np.allclose(t_parameters[\"b2\"], np.array([\n",
    "    [-0.84610769]\n",
    "]))"
   ],
   "id": "fd838eaa60909e51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Update parameters (with dropout)",
   "id": "b6e944017016fad1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def update_parameters_dropout(params, grads, learning_rate):\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 3\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ],
   "id": "b173d8883177264b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "a0b255e0e4bb1326"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "from common import CV_DATA_DIR"
   ],
   "id": "85f1d7647f1c1549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"train_catvnoncat.h5\", \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File(CV_DATA_DIR / \"playground\" / \"dnn\" / \"test_catvnoncat.h5\", \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ],
   "id": "b47814d4d1ea5ddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()",
   "id": "8b0fac50f4cdd9e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print (\"Number of training examples:\", train_x_orig.shape[0])\n",
    "print (\"Number of testing examples:\", test_x_orig.shape[0])\n",
    "print (\"Each image is of size: (\" + str(train_x_orig.shape[1]) + \", \" + str(train_x_orig.shape[1]) + \", 3)\")\n",
    "\n",
    "assert train_x_orig.shape == (209, 64, 64, 3)\n",
    "assert train_y.shape == (1, 209)\n",
    "assert test_x_orig.shape == (50, 64, 64, 3)\n",
    "assert test_y.shape == (1, 50)"
   ],
   "id": "a0357974981bd6fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print(\"train_x's shape: \" + str(train_x.shape))\n",
    "print(\"test_x's shape: \" + str(test_x.shape))"
   ],
   "id": "fb1cacb5af286809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train L-layer model",
   "id": "9dd9c9e0bbc54fd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train L-layer Model (without dropout)",
   "id": "9ef987cdffdad827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learning_rate = 0.0075\n",
    "\n",
    "layers_config = [\n",
    "\t{\"dims\": 12288, \"activation\": None},\n",
    "\t{\"dims\": 20, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 7, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 5, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "id": "5ec28d71255f185a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def L_layer_model(X, Y, layers, learning_rate = 0.0075, num_iterations = 3500, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters(layers)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward_propagate(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagate(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ],
   "id": "2557736fb8cfb5c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parameters, costs = L_layer_model(\n",
    "    train_x, train_y,\n",
    "    layers_config,\n",
    "    learning_rate,\n",
    "    num_iterations = 4500,\n",
    "    print_cost = True)"
   ],
   "id": "d1bb96e07147bb37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_costs(costs, learning_rate)",
   "id": "f320a01b93f03f9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train L-layer Model (with dropout)",
   "id": "988e7967aef48e2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "learning_rate = 0.0075\n",
    "\n",
    "layers_config_dropout = [\n",
    "\t{\"dims\": 12288, \"keep_prob\": 1.0, \"activation\": None},\n",
    "\t{\"dims\": 20, \"keep_prob\": 0.95, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 7, \"keep_prob\": 0.98, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 5, \"keep_prob\": 1.0, \"activation\": \"relu\"},\n",
    "\t{\"dims\": 1, \"keep_prob\": 1.0, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "id": "3d6522f3cc982d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def L_layer_model_dropout(X, Y, layers, learning_rate = 0.0075, num_iterations = 3500, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters_dropout(layers)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward_propagate_dropout(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagate_dropout(AL, Y, parameters, caches)\n",
    "        parameters = update_parameters_dropout(parameters, grads, learning_rate)\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ],
   "id": "5e59dfdc08c6b530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parameters_dropout, costs_dropout = L_layer_model_dropout(\n",
    "    train_x, train_y,\n",
    "    layers_config_dropout,\n",
    "    num_iterations = 4500,\n",
    "    print_cost = True)"
   ],
   "id": "aa81b270d7de26d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_costs(costs_dropout, learning_rate)",
   "id": "f214f5bc8b52fde4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "f0f703bd86e86840"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction (without dropout)",
   "id": "215745db1dc064e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = forward_propagate(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    accuracy = np.sum((p == y) / m)\n",
    "    return p, accuracy"
   ],
   "id": "b8c62dcbe48c36c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, accuracy = predict(train_x, train_y, parameters)\n",
    "print(\"Accuracy (train):\", accuracy)\n",
    "_, accuracy = predict(test_x, test_y, parameters)\n",
    "print(\"Accuracy (test):\", accuracy)"
   ],
   "id": "4da2e9c768b9542c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prediction (with dropout)",
   "id": "a25962460149e6bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_dropout(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches = forward_propagate_dropout(X, parameters)\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    accuracy = np.sum((p == y) / m)\n",
    "    return p, accuracy"
   ],
   "id": "1fcb122c98491911",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, accuracy_dropout = predict_dropout(train_x, train_y, parameters_dropout)\n",
    "print(\"Accuracy (train):\", accuracy_dropout)\n",
    "_, accuracy_dropout = predict_dropout(test_x, test_y, parameters_dropout)\n",
    "print(\"Accuracy (test):\", accuracy_dropout)"
   ],
   "id": "857f963c1177a086",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gradient Checking\n",
    "\n",
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and loss function.\n",
    "\n",
    "Definition of a derivative (or gradient):$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$$\n",
    "\n",
    "For each i in parameters:\n",
    "- Compute `J_p[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(param_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagate(X, vec_to_dic(`$\\theta^{+}$`, param_shapes))`.\n",
    "- To compute `J_m[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Where gradapprox[i] is an approximation of the gradient with respect to `param_values[i]`. Then comparing gradapprox vector with the gradients vector from backpropagation the difference can be received:\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n",
    "\n",
    "$10^{-7}$ or less is good, $10^{-5}$ need to double-check components, $10^{-3}$ - need to worry."
   ],
   "id": "17ec7d639912c1ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dic_to_vec(parameters):\n",
    "    theta = []\n",
    "    shapes = {}\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]:\n",
    "        shapes[key] = parameters[key].shape\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta, shapes"
   ],
   "id": "1d510081dcbae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vec_to_dic(param_values, param_shapes):\n",
    "    parameters = {}\n",
    "    offset = 0\n",
    "    for key,shape in param_shapes.items():\n",
    "        items = shape[0] * shape[1]\n",
    "        parameters[key] = param_values[offset:offset+items].reshape(shape)\n",
    "        offset += items\n",
    "    return parameters"
   ],
   "id": "c0a9985375c85326",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def grad_to_vec(gradients):\n",
    "    theta = []\n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\", \"dW4\", \"db4\"]:\n",
    "        new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count += 1\n",
    "    return theta"
   ],
   "id": "27bf93c91c41ec5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gradient_check(parameters, gradients, X, Y, N=100, epsilon=1e-7):\n",
    "    param_values, param_shapes = dic_to_vec(parameters)\n",
    "    grads = grad_to_vec(gradients)\n",
    "\n",
    "    No = param_values.shape[0]\n",
    "    J_p = np.zeros((No, 1))\n",
    "    J_m = np.zeros((No, 1))\n",
    "    gradapproxs = np.zeros((No, 1))\n",
    "\n",
    "    assert N <= No, \\\n",
    "        \"N parameter is invalid\"\n",
    "\n",
    "    for i in range(N):\n",
    "        theta_p = np.copy(param_values)\n",
    "        theta_p[i] = theta_p[i] + epsilon\n",
    "        Yp_hat, _ = forward_propagate(X, vec_to_dic(theta_p, param_shapes))\n",
    "        J_p[i] = compute_cost(Yp_hat, Y)\n",
    "\n",
    "        theta_m = np.copy(param_values)\n",
    "        theta_m[i] = theta_m[i] - epsilon\n",
    "        Ym_hat, _ = forward_propagate(X, vec_to_dic(theta_m, param_shapes))\n",
    "        J_m[i] = compute_cost(Ym_hat, Y)\n",
    "\n",
    "        gradapproxs[i] = (J_p[i] - J_m[i]) / (2 * epsilon)\n",
    "\n",
    "    num = np.linalg.norm(grads[:N] - gradapproxs[:N])\n",
    "    den = np.linalg.norm(grads[:N]) + np.linalg.norm(gradapproxs[:N])\n",
    "    differences = num / den\n",
    "    return differences\n"
   ],
   "id": "3258e01cfaf240e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parameters = initialize_parameters(layers_config)\n",
    "AL, caches = forward_propagate(train_x, parameters)\n",
    "gradients = backward_propagate(AL, train_y, caches)\n",
    "\n",
    "difference = gradient_check(parameters, gradients, train_x, train_y)\n",
    "print(\"Difference:\", difference)"
   ],
   "id": "ad3bb3ae1f82e8bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
