{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from common import CV_DATA_DIR\n",
    "\n",
    "WWD_PATH = os.path.join(CV_DATA_DIR, \"playground\", \"audio\", \"wwd\")"
   ],
   "id": "f7fc0cc6b2688cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TF Version: {tf.__version__}\")\n",
    "print(f\"TF Devices: {[d.device_type for d in tf.config.list_physical_devices()]}\")"
   ],
   "id": "80cf7450db6b0929",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import IPython\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parameters",
   "id": "321651fb9554929a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SAMPLE_LEN = 10000\n",
    "Tx = 5511\n",
    "Ty = 1375\n",
    "n_freq = 101"
   ],
   "id": "e37233f607d08fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Samples",
   "id": "6558cf71a17f2f1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_raw_audio(path):\n",
    "    backgrounds = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "\n",
    "    dir = os.path.join(path, \"backgrounds\")\n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            background = AudioSegment.from_wav(os.path.join(dir, filename))\n",
    "            backgrounds.append(background)\n",
    "    dir = os.path.join(path, \"positives\")\n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            positive = AudioSegment.from_wav(os.path.join(dir, filename))\n",
    "            positives.append(positive)\n",
    "    dir = os.path.join(path, \"negatives\")\n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith(\"wav\"):\n",
    "            negative = AudioSegment.from_wav(os.path.join(dir, filename))\n",
    "            negatives.append(negative)\n",
    "\n",
    "    return positives, negatives, backgrounds"
   ],
   "id": "1731652ba8442fb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load RAW audio files\n",
    "positives, negatives, backgrounds = load_raw_audio(WWD_PATH)"
   ],
   "id": "99a1210b3b580",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The number of backgrounds samples\n",
    "BACKGROUNDS_NUM = len(backgrounds)\n",
    "# The number of positive samples\n",
    "POSITIVES_NUM = len(positives)\n",
    "# The number of negative samples\n",
    "NEGATIVES_NUM = len(negatives)"
   ],
   "id": "afcb4d7dacb955b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Background lengths (total {len(backgrounds)}): \\n...{[len(b) for b in backgrounds]}\")\n",
    "print(f\"Positive lengths (total {len(positives)}): \\n...{[len(a) for a in positives]}\")\n",
    "print(f\"Negative lengths (total {len(negatives)}): \\n...{[len(n) for n in negatives]}\")"
   ],
   "id": "604b5ff295804ddb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Spectrogram\n",
    "\n",
    "* Audio recording is a long list of numbers measuring the little air pressure changes detected by the microphone\n",
    "* Audio sample rate of 44100 Hz means 44100 numbers of pressure value per second\n",
    "* Spectrogram tells how much different frequencies are present in an audio clip at any moment in time\n",
    "* Spectrogram is computed by sliding a window over the raw audio signal and calculating the most active frequencies in each window"
   ],
   "id": "85a453056b6a62e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_wav_info(wav_file):\n",
    "    rate, data = wavfile.read(wav_file)\n",
    "    return rate, data"
   ],
   "id": "d8d2a72b5f69009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def match_target_amplitude(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)"
   ],
   "id": "6f86385240e5f8c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_spectrogram(wav_file, nfft=200, fs=8000, noverlap=120):\n",
    "    \"\"\"\n",
    "    Compute spectrogram from WAV file.\n",
    "    :param wav_file: the path of wav file\n",
    "    :param nfft: the length of each window segment\n",
    "    :param fs: the amount of sampling frequencies\n",
    "    :param noverlap: the overlap between window segments\n",
    "    :return: the periodic spectrogram\n",
    "    \"\"\"\n",
    "    _, data = get_wav_info(wav_file)\n",
    "    nchannels = data.ndim\n",
    "    if nchannels == 2:\n",
    "        data = data[:,0]\n",
    "    pxx, freqs, bins, im = plt.specgram(data, NFFT=nfft, Fs=fs, noverlap=noverlap)\n",
    "    return pxx"
   ],
   "id": "18dcaa63bfc4874a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EXAMPLE1 = os.path.join(WWD_PATH, \"examples\", \"example_train.wav\")\n",
    "\n",
    "# Compute spectrogram of audio file\n",
    "Pxx = get_spectrogram(EXAMPLE1)"
   ],
   "id": "91cf86e9eeebfc81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The color in spectrogram show the degree to which different frequencies are present\n",
    " * green - denotes more active\n",
    " * blue - denotes less active frequencies"
   ],
   "id": "d9fb55945d94a348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert Pxx.shape[0] == n_freq, \\\n",
    "    \"Invalid number of frequencies\"\n",
    "assert Pxx.shape[1] == Tx, \\\n",
    "    \"Invalid number of spectrogram time steps\""
   ],
   "id": "47876cbd53b4a234",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sampling\n",
    "\n",
    "* Recording audio clips is slow and ineffective\n",
    "* Easier to record a lot of audio clips with positive and negative words on top of background noise\n",
    "* To generate audio clip we need:\n",
    "    * Pick a random background audio clip\n",
    "    * Randomly insert 0-n audio clips of positive word\n",
    "    * Randomly insert 0-m audio clips of negative words\n",
    "* Generating audio clips provides easy way to generate labels $y^{\\langle t \\rangle}$"
   ],
   "id": "6144b406007bec5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in audio clip.\n",
    "    :param segment_ms: the duration of the audio clip in ms\n",
    "    :return: a tuple of (segment_start, segment_end) in ms\n",
    "    \"\"\"\n",
    "    segment_start = np.random.randint(low=0, high=SAMPLE_LEN-segment_ms)\n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    return segment_start, segment_end"
   ],
   "id": "e9d6de34a42b72a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "    :param segment_time: a tuple of (segment_start, segment_end) for the new segment\n",
    "    :param previous_segments: a list of tuples of (segment_start, segment_end) for the existing segments\n",
    "    :return: True if the time segment overlaps with any of the existing segments, False otherwise\n",
    "    \"\"\"\n",
    "    segment_start, segment_end = segment_time\n",
    "    overlap = False\n",
    "    for previous_start, previous_end in previous_segments:\n",
    "        if segment_start <= previous_end and segment_end >= previous_start:\n",
    "            overlap = True\n",
    "            break\n",
    "    return overlap"
   ],
   "id": "237ef492f45f16c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def insert_audio_clip(background, audio_clip, previous_segments, attempts=5):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the\n",
    "    audio segment does not overlap with existing segments.\n",
    "    :param background: the background audio recording.\n",
    "    :param audio_clip: the audio clip to be inserted/overlaid.\n",
    "    :param previous_segments: times when audio segments have already been placed\n",
    "    :param attempts: the number of attempts to find where to insert audio clip\n",
    "    :return: the updated background audio\n",
    "    \"\"\"\n",
    "    segment_ms = len(audio_clip)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    retry_cnt = attempts\n",
    "\n",
    "    while is_overlapping(segment_time, previous_segments) and retry_cnt >= 0:\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "        retry_cnt -= 1\n",
    "\n",
    "    if not is_overlapping(segment_time, previous_segments):\n",
    "        previous_segments.append(segment_time)\n",
    "        new_background = background.overlay(audio_clip, position=segment_time[0])\n",
    "    else:\n",
    "        new_background = background\n",
    "        segment_time = (SAMPLE_LEN, SAMPLE_LEN)\n",
    "\n",
    "    return new_background, segment_time"
   ],
   "id": "2c4d58ff5870e601",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def insert_ones(y, segment_end_ms, count=50):\n",
    "    \"\"\"\n",
    "    Update the label vector y. The labels of the `count` output steps strictly after the end of the segment should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the `count` following labels should be ones.\n",
    "    :param y: the labels to modify\n",
    "    :param segment_end_ms: the end of the segment in ms\n",
    "    :param count: the number of labels to set 1\n",
    "    :return: the modified labels\n",
    "    \"\"\"\n",
    "    _, Ty = y.shape\n",
    "    segment_end_y = int(segment_end_ms * Ty / float(SAMPLE_LEN))\n",
    "    if segment_end_y < Ty:\n",
    "        y[0, segment_end_y + 1:segment_end_y + count + 1] = 1\n",
    "    return y"
   ],
   "id": "6becdbed5cb3b619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_training_example(background, positives, negatives, max_positives=5, max_negatives=2):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, activates, and negatives.\n",
    "    :param background: a 10-second background audio recording\n",
    "    :param positives: a list of audio segments of the word \"activate\"\n",
    "    :param negatives: a list of audio segments of random words that are not \"activate\"\n",
    "    :return:\n",
    "    x -- the spectrogram of the training example\n",
    "    y -- the label at each time step of the spectrogram\n",
    "    \"\"\"\n",
    "\n",
    "    # Make background quieter\n",
    "    background = background - 20\n",
    "\n",
    "    y = np.zeros((1, Ty))\n",
    "    previous_segments = []\n",
    "\n",
    "    # Select 0-5 random positive audio clips from the entire list\n",
    "    number_of_positives = np.random.randint(0, max_positives)\n",
    "    random_indices = np.random.randint(len(positives), size=number_of_positives)\n",
    "    random_positives = [positives[i] for i in random_indices]\n",
    "    for positive in random_positives:\n",
    "        background, segment_time = insert_audio_clip(background, positive, previous_segments)\n",
    "        segment_start, segment_end = segment_time\n",
    "        y = insert_ones(y, segment_end)\n",
    "\n",
    "    # Select 0-2 random negative audio clips from the entire list\n",
    "    number_of_negatives = np.random.randint(0, max_negatives)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "    for negative in random_negatives:\n",
    "        background, _ = insert_audio_clip(background, negative, previous_segments)\n",
    "\n",
    "    train_file = os.path.join(\"data\", \"train.wav\")\n",
    "    background = match_target_amplitude(background, -20.0)\n",
    "    _ = background.export(train_file, format=\"wav\")\n",
    "    x = get_spectrogram(train_file)\n",
    "\n",
    "    return x, y"
   ],
   "id": "aa08086e7b494ab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate one example\n",
    "x, y = create_training_example(backgrounds[0], positives, negatives)"
   ],
   "id": "9129ca8d327b464c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Audio after overlaying positive and negative audio clips\n",
    "IPython.display.Audio(os.path.join(\"data\", \"train.wav\"))"
   ],
   "id": "888a983f707f65ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Origin Audio clip\n",
    "IPython.display.Audio(os.path.join(WWD_PATH, \"examples\", \"example_train.wav\"))"
   ],
   "id": "3056574cab3ad0b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The Y labels of modified audio clip\n",
    "plt.plot(y[0])"
   ],
   "id": "13e6e1020d5365ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Generating samples (optional)__",
   "id": "3ab6f9a60fa099c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The number of samples to generate\n",
    "n_samples = 1000\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(0, n_samples):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    x, y = create_training_example(backgrounds[i % 2], positives, negatives)\n",
    "    # Spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    X_train.append(x.swapaxes(0,1))\n",
    "    Y_train.append(y.swapaxes(0,1))\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ],
   "id": "722d45359acf5c28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the data for further uses\n",
    "np.save(os.path.join(WWD_PATH, \"XY_train\", \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(WWD_PATH, \"XY_train\", \"Y_train.npy\"), Y_train)"
   ],
   "id": "3ef5b97db200e6d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Load previously generated samples__",
   "id": "6f5a6597fc945a58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training set\n",
    "X_train = np.load(os.path.join(WWD_PATH, \"XY_train\", \"X_train.npy\"))\n",
    "Y_train = np.load(os.path.join(WWD_PATH, \"XY_train\", \"Y_train.npy\"))"
   ],
   "id": "47a72c1d7d325343",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"X (training) shape: {X_train.shape}\")\n",
    "print(f\"Y (training) shape: {Y_train.shape}\")"
   ],
   "id": "d8feecb1ad5e65de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Validation set\n",
    "X_dev = np.load(os.path.join(WWD_PATH, \"XY_dev\", \"X_dev.npy\"))\n",
    "Y_dev = np.load(os.path.join(WWD_PATH, \"XY_dev\", \"Y_dev.npy\"))"
   ],
   "id": "af2349b956646670",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"X (dev) shape: {X_dev.shape}\")\n",
    "print(f\"Y (dev) shape: {Y_dev.shape}\")"
   ],
   "id": "96dcd026b5cb846",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "6d36426a047ecd49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    :param input_shape: shape of the model's input data (using Keras conventions)\n",
    "    :return: Keras model instance\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv1D(filters=196,kernel_size=15,strides=4),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation(activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(rate=0.8),\n",
    "        tf.keras.layers.GRU(units=128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(rate=0.8),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.GRU(units=128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(rate=0.8),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.8),\n",
    "        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    ])"
   ],
   "id": "dc248d781181b6d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Create model__",
   "id": "1ad4b97a7a316362"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = create_model(input_shape = (Tx, n_freq))",
   "id": "cf75673087bca0c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6, beta_1=0.9, beta_2=0.999),\n",
    "    metrics=[\"accuracy\"])"
   ],
   "id": "556e8a698dce86ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Fit model__",
   "id": "59dadbce0c6a2ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit the neural network\n",
    "history = model.fit(X_train, Y_train, batch_size=4, epochs=64)"
   ],
   "id": "dad79e9bde6c252b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss = history.history[\"loss\"]\n",
    "epochs = len(loss)\n",
    "\n",
    "plt.plot(range(len(loss)), loss, 'r', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ],
   "id": "4419e0ef80a9dcc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss, acc, = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ],
   "id": "c7493e4e7e436396",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Predicting__",
   "id": "bb5e59d414e2bb31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chime_file = os.path.join(WWD_PATH, \"examples\", \"chime.wav\")",
   "id": "42ced03649516245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def detect_triggerword(model, filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "\n",
    "    # Correct the amplitude of the input file before prediction\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    audio_clip = match_target_amplitude(audio_clip, -20.0)\n",
    "    temp_file = os.path.join(\"data\", \"temp.wav\")\n",
    "    _ = audio_clip.export(temp_file, format=\"wav\")\n",
    "\n",
    "    x = get_spectrogram(temp_file)\n",
    "    # Spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions"
   ],
   "id": "c5f0498bde9590a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    consecutive_timesteps = 0\n",
    "    i = 0\n",
    "    while i < Ty:\n",
    "        consecutive_timesteps += 1\n",
    "        if consecutive_timesteps > 20:\n",
    "            audio_clip = audio_clip.overlay(chime, position=((i / Ty) * audio_clip.duration_seconds) * 1000)\n",
    "            consecutive_timesteps = 0\n",
    "            i = 75 * (i // 75 + 1)\n",
    "            continue\n",
    "        if predictions[0, i, 0] < threshold:\n",
    "            consecutive_timesteps = 0\n",
    "        i += 1\n",
    "\n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ],
   "id": "23f6c19bfa1467ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "FILE1 = os.path.join(WWD_PATH, \"dev\", \"2.wav\")\n",
    "prediction = detect_triggerword(model, FILE1)\n",
    "chime_on_activate(FILE1, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ],
   "id": "5a354b305c345e9c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
