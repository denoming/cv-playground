{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TensorFlow: Text Pre-Processing",
   "id": "1437c550d8086298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import io\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "24487c47128e3d65",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"TF Eager mode: \", tf.executing_eagerly())\n",
    "print(\"TF GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Vectorization",
   "id": "fc11a7bddfe0a707"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define corpus sentences\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat!\",\n",
    "    \"Do you thing my dog is amazing?\"\n",
    "]"
   ],
   "id": "f65b75bcc9524297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Padding vectorization__",
   "id": "fdf9324e8f065bac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vectorize_layer0 = tf.keras.layers.TextVectorization()\n",
    "vectorize_layer0.adapt(sentences)\n",
    "vocabulary0 = vectorize_layer0.get_vocabulary()"
   ],
   "id": "f06c94d98020d2e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 0 corresponds to padding (used to pad shorter sentences)\n",
    "# 1 corresponds to unknown word given existing vocabulary\n",
    "_ = [print(\"Index={}, Value={}\".format(index, value)) for index, value in enumerate(vocabulary0)]"
   ],
   "id": "7e1717ffa4d0544",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sentences_with_oov = sentences + [\"I really love my dog\"]\n",
    "\n",
    "# Map sentences to token sequences (without padding)\n",
    "seq_ds = tf.data.Dataset.from_tensor_slices(sentences_with_oov)\n",
    "_ = [print(\"{} => {}\".format(t, s)) for t, s in zip(sentences_with_oov, seq_ds.map(vectorize_layer0))]"
   ],
   "id": "62e4a21dde9afff9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Map sentences to token sequences (with padding)\n",
    "_ = [print(\"{} => {}\".format(t, s)) for t, s in zip(sentences_with_oov, vectorize_layer0(sentences_with_oov))]"
   ],
   "id": "d36707f3492de599",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Ragged vectorization__",
   "id": "b3411b2815ca0aaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vectorize_layer1 = tf.keras.layers.TextVectorization(ragged=True)\n",
    "vectorize_layer1.adapt(sentences)\n",
    "vocabulary1 = vectorize_layer1.get_vocabulary()"
   ],
   "id": "1d7bf91961ed49b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Map sentences to token sequences (without padding)\n",
    "ragged_sequences = vectorize_layer1(sentences)\n",
    "_ = [print(\"{} => {}\".format(t, s)) for t, s in zip(sentences, ragged_sequences)]"
   ],
   "id": "eb452f4ae8de0ddf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pre-pad the sequences of the ragged tensor\n",
    "print(tf.keras.utils.pad_sequences(ragged_sequences.numpy()))"
   ],
   "id": "c0da79800e42818c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Creating word embeddings using NN and `imdb_ewviews` dataset.\n",
    "Dataset contains positive or negative film reviews. Therefore, training classification network provides weights from `Embedding` layer."
   ],
   "id": "c9e893001df5d5e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The size of vocabulary\n",
    "VOCAB_SIZE = 10000\n",
    "# The max length of sequence after word vectorization\n",
    "MAX_LENGTH = 120\n",
    "# The length of word embedding\n",
    "EMBEDDING_DIM = 16"
   ],
   "id": "64a3b319a10b8625",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "(train_ds, test_ds), info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    data_dir=\"data\",\n",
    "    download=True)"
   ],
   "id": "30471c3eeeb3ba4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(info)",
   "id": "1a4c108de616ca0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Element specification: (review, label)\n",
    "print(train_ds.element_spec)"
   ],
   "id": "d8d7836c49b494a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the string inputs and integer outputs of the training se\n",
    "train_reviews = train_ds.map(lambda review, label: review)\n",
    "train_labels = train_ds.map(lambda review, label: label)\n",
    "\n",
    "# Get the string inputs and integer outputs of the test set\n",
    "test_reviews = test_ds.map(lambda review, label: review)\n",
    "test_labels = test_ds.map(lambda review, label: label)"
   ],
   "id": "c23bf3181ebd4e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate the vectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Generate the vocabulary based only on the training set\n",
    "vectorize_layer.adapt(train_reviews)"
   ],
   "id": "42efb0dc10151dba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply vectorization (sentences to sequences)\n",
    "train_seqs = train_reviews.map(lambda text: vectorize_layer(text))\n",
    "test_seqs = test_reviews.map(lambda text: vectorize_layer(text))\n",
    "\n",
    "# Zip sequences together with labels\n",
    "train_ds_vec = tf.data.Dataset.zip(train_seqs,train_labels)\n",
    "test_ds_vec = tf.data.Dataset.zip(test_seqs, test_labels)"
   ],
   "id": "106d4d2ca2cb6fa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get vocabulary\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Get a sample integer sequences\n",
    "sample_sequence = train_seqs.take(1).get_single_element()\n",
    "\n",
    "# Lookup each token in the vocabulary\n",
    "decoded_text = \" \".join([vocabulary[token] for token in sample_sequence])\n",
    "\n",
    "# Print decoded text together with [UNK]\n",
    "print(decoded_text)"
   ],
   "id": "37e11446ec7e6092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Optimize the datasets for training\n",
    "train_ds_final = (train_ds_vec\n",
    "    .cache()\n",
    "    .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "    .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "test_ds_final = (test_ds_vec\n",
    "    .cache()\n",
    "    .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ],
   "id": "97d6fb6b5ca7a42b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(MAX_LENGTH,)),\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ],
   "id": "c744cf97ee33efdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds_final,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=test_ds_final,\n",
    "    verbose=2)"
   ],
   "id": "4c86e9a19a8eeb7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "# Plot the accuracy and loss\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ],
   "id": "cb6cf737ed782f8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the embedding layer from the model (i.e. first layer)\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights of the embedding layer\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
    "print(embedding_weights.shape)"
   ],
   "id": "c1460f7485648130",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "log_dir = \"logs/imdb\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "out_v = io.open(os.path.join(log_dir, \"vecs.tsv\"), \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(os.path.join(log_dir, \"meta.tsv\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "for word_num in range(1, len(vocabulary)):\n",
    "    word_name = vocabulary[word_num]\n",
    "    word_embedding = embedding_weights[word_num]\n",
    "    out_m.write(word_name + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ],
   "id": "7e3837c9480723eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Open [Tensorflow Embedding Projector](https://projector.tensorflow.org/) and load the two files\n",
    "`logs/imdb/vecs.tsv` (Step 1) and `logs/imdb/meta.tsv` (Step 2) to see the visualization."
   ],
   "id": "393a32835b02d89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Subword Embeddings",
   "id": "ef843d61ba31e02c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import keras_nlp",
   "id": "c4486f8bb1a7455a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "(train_ds, test_ds), info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    data_dir=\"data\",\n",
    "    download=True)"
   ],
   "id": "b293a78469be2cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the string inputs and integer outputs of the training se\n",
    "train_reviews = train_ds.map(lambda review, label: review)\n",
    "train_labels = train_ds.map(lambda review, label: label)\n",
    "\n",
    "# Get the string inputs and integer outputs of the test set\n",
    "test_reviews = test_ds.map(lambda review, label: review)\n",
    "test_labels = test_ds.map(lambda review, label: label)"
   ],
   "id": "c36cdc6cc75b8c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the subword vocabulary and save to a file\n",
    "keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    train_reviews,\n",
    "    vocabulary_size=8000,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    vocabulary_output_file='data/imdb_vocab_subwords.txt'\n",
    ")"
   ],
   "id": "7fcf9fc4b603a396",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
