{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TensorFlow: Sentiment classification using word embeddings",
   "id": "1440ed014edccdce"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ],
   "id": "aba6b5df7be579a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare Datasets",
   "id": "4abdb361c490f25e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load datasets",
   "id": "95b99d2146b9c94b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "(raw_train_ds, raw_val_ds, raw_test_ds), ds_info = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:60%]\", \"train[60%:]\", \"test\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ],
   "id": "66c48f275c276fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pre-process Datasets",
   "id": "1165c01aa6c28205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The size of the vocabulary\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "# The sequence length (pad or truncate to this value)\n",
    "SEQ_LEN = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=MAX_FEATURES,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQ_LEN)"
   ],
   "id": "5c39718058d53890",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_x = raw_train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Adapt vectorization layer (train data must be used only)\n",
    "vectorize_layer.adapt(train_x)"
   ],
   "id": "71c907a2f56fe394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vectorize_text(x, y):\n",
    "    # Add a batch dimension\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    y = tf.expand_dims(y, -1)\n",
    "    # Vectorize a text\n",
    "    return vectorize_layer(x), y"
   ],
   "id": "2fb689b5d417a342",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x, y = next(iter(raw_train_ds.take(1)))\n",
    "print(\"Review: \", x.numpy())\n",
    "print(\"Label: \", y.numpy())\n",
    "print(\"Vectorized review: \", vectorize_text(x, y))"
   ],
   "id": "43910e0be3c1ffbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"3201 ---> \",vectorize_layer.get_vocabulary()[1177])\n",
    "print(\"2194 ---> \",vectorize_layer.get_vocabulary()[7819])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ],
   "id": "c37159c51d62d3d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_ds = (raw_train_ds.map(vectorize_text)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE))\n",
    "val_ds = (raw_val_ds.map(vectorize_text)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE))\n",
    "test_ds = (raw_test_ds.map(vectorize_text)\n",
    "   .cache()\n",
    "   .prefetch(tf.data.AUTOTUNE))"
   ],
   "id": "ee8e4039806d3329",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Model",
   "id": "80bbdb908f68f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Model",
   "id": "eba22008d42c4037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EMBEDDING_DIM = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Creates embedded vector for each word-index\n",
    "    # (batch_size, steps) -> (batch_size, steps, features)\n",
    "    layers.Embedding(MAX_FEATURES, EMBEDDING_DIM),\n",
    "    layers.Dropout(0.2),\n",
    "    # Dimensionality reduction by averaging over feature1,...,featureN across all steps (columns)\n",
    "    # (batch_size, steps, features) -> (batch_size, features)\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ],
   "id": "f0c4ae655e91d77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "9ffcffcfa80d481a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)]\n",
    ")"
   ],
   "id": "32f9dd0960e10f5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fit Model",
   "id": "b19b31352e3e6f0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)],\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "9e335b68ebe677aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate Model",
   "id": "267b89676570b4f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ],
   "id": "ff369cd755ae444c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc = history.history[\"binary_accuracy\"]\n",
    "val_acc = history.history[\"val_binary_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"b\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, \"g\", label=\"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, loss, \"r\", label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, \"g\", label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "e4d3ea3db19b803d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Export Model",
   "id": "d0d5ec9614428c42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create export model with text vectorization layer\n",
    "# (such movel is able to get into input raw text)\n",
    "export_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    model,\n",
    "    layers.Activation(\"sigmoid\"),\n",
    "])"
   ],
   "id": "14ed1c20e70fe59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "export_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "id": "a3c73ff75a913380",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "export_model.summary()",
   "id": "183835f050b9e44f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add batch dimension to x and y\n",
    "export_test_ds = raw_test_ds.map(lambda x, y: (tf.expand_dims(x, -1), tf.expand_dims(y, -1)))"
   ],
   "id": "30a056f54ea44d95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate exported model on unseen test data\n",
    "metrics = export_model.evaluate(export_test_ds, return_dict=True)\n",
    "print(metrics)"
   ],
   "id": "49ea0095cf5b387e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predict on the examples using exported model\n",
    "examples = tf.constant([\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "])\n",
    "\n",
    "export_model.predict(examples)"
   ],
   "id": "986de69cac4ec51f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
