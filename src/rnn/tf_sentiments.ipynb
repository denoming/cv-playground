{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TensorFlow: Sentiment classification using word embeddings",
   "id": "1440ed014edccdce"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "import os",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from tensorboard import program\n",
    "\n",
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"TF Eager mode: \", tf.executing_eagerly())\n",
    "print(\"TF GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")"
   ],
   "id": "aba6b5df7be579a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set dir for logging\n",
    "LOG_ROOT_DIR = os.path.join(\"logs\", \"sentiments\")\n",
    "# Set the size of buffer for samples shuffling\n",
    "BUFFER_SIZE = 10_000\n",
    "# Set the size of batches\n",
    "BATCH_SIZE = 32\n",
    "# The size of the vocabulary\n",
    "MAX_FEATURES = 10_000\n",
    "# The sample sequence length (truncate or pad to get this length)\n",
    "SEQ_LEN = 250\n",
    "# Set the number of dimensions for embedded vectors\n",
    "EMBEDDING_DIM = 64"
   ],
   "id": "47f9b6e1798d0922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare Datasets",
   "id": "4abdb361c490f25e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load datasets",
   "id": "95b99d2146b9c94b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataset from TFDS collection\n",
    "(raw_train_ds, raw_val_ds, raw_test_ds), ds_info = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:80%]\", \"train[80%:]\", \"test\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ],
   "id": "66c48f275c276fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print one sample\n",
    "for sample, label in raw_train_ds.take(1):\n",
    "    print(f\"Text: {sample.numpy()}\")\n",
    "    print(f\"Label: {label.numpy()}\")"
   ],
   "id": "774a4def4c65d2cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pre-process Datasets",
   "id": "1165c01aa6c28205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_ds = (raw_train_ds\n",
    "            .shuffle(BUFFER_SIZE)\n",
    "            .repeat()\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds = (raw_val_ds\n",
    "           .batch(BATCH_SIZE)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "test_ds = (raw_test_ds\n",
    "           .batch(BATCH_SIZE)\n",
    "           .prefetch(tf.data.AUTOTUNE))"
   ],
   "id": "6d53b99e1a985774",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_FEATURES,\n",
    "    output_sequence_length=SEQ_LEN\n",
    ")"
   ],
   "id": "5c39718058d53890",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adapt vectorization layer (ony train data must be used)\n",
    "vectorize_layer.adapt(raw_train_ds.map(lambda x, y: x))"
   ],
   "id": "71c907a2f56fe394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Test text vectorization__",
   "id": "bbcc39c4eb2a0ae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vectorize_text(x, y):\n",
    "    # Add a batch dimension\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    y = tf.expand_dims(y, -1)\n",
    "    # Vectorize a text\n",
    "    return vectorize_layer(x), y"
   ],
   "id": "2fb689b5d417a342",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x, y = next(iter(raw_train_ds.take(1)))\n",
    "print(\"Review: \", x.numpy())\n",
    "print(\"Label: \", y.numpy())\n",
    "print(\"Vectorized review: \", vectorize_text(x, y))"
   ],
   "id": "43910e0be3c1ffbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"152 ---> \", vectorize_layer.get_vocabulary()[152])\n",
    "print(\"113 ---> \", vectorize_layer.get_vocabulary()[113])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ],
   "id": "c37159c51d62d3d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Model",
   "id": "80bbdb908f68f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the amount of steps per epoch\n",
    "STEPS_PER_EPOCH = int(raw_train_ds.cardinality() // BATCH_SIZE)"
   ],
   "id": "6f37a0dec19ec21f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define learning rate scheduler\n",
    "lr_scheduler = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    1e-4,\n",
    "    decay_steps=STEPS_PER_EPOCH*10, # Decay every 10 epochs\n",
    "    decay_rate=0.5,\n",
    "    staircase=True)"
   ],
   "id": "f27695155d856147",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model - CNN",
   "id": "4cad373dc5dd0d70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MODEL0_LOGS = os.path.join(LOG_ROOT_DIR, \"tb-model0\")",
   "id": "e5f0948e28d6cc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Model",
   "id": "9e8214b9adb595e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model0 = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    # Creates embedded vector for each word-index\n",
    "    # (batch_size, steps) -> (batch_size, steps, features)\n",
    "    tf.keras.layers.Embedding(input_dim=MAX_FEATURES, output_dim=EMBEDDING_DIM),\n",
    "    # Regularize by randomly dropping dimensions in feature vector\n",
    "    tf.keras.layers.SpatialDropout1D(0.5),\n",
    "    # Conv1D + global max pooling\n",
    "    tf.keras.layers.Conv1D(64, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(64, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    # Dense Layer\n",
    "    tf.keras.layers.Dense(\n",
    "        units=64,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model0.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_scheduler),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ],
   "id": "a03f13fb28f16439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fit Model",
   "id": "eb01094ac7e2f52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history0 = model0.fit(\n",
    "    train_ds,\n",
    "    epochs=15,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3, verbose=1),\n",
    "        tf.keras.callbacks.TensorBoard(MODEL0_LOGS, histogram_freq=1)\n",
    "    ],\n",
    "    validation_data=val_ds,\n",
    "    verbose=2\n",
    ")"
   ],
   "id": "fdf71ef3ba9950c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluate Model",
   "id": "396af498e6f76e63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = model0.evaluate(test_ds)\n",
    "\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ],
   "id": "f46b44bf1aa162c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tb0 = program.TensorBoard()\n",
    "tb0.configure(argv=[None, '--load_fast', 'false', '--logdir', MODEL0_LOGS])\n",
    "url = tb0.launch()\n",
    "print(f\"TensorBoard listening on {url}\")"
   ],
   "id": "89edf037a9a05672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 1 - GRU",
   "id": "e5140c8e466491ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MODEL1_LOGS = os.path.join(LOG_ROOT_DIR, \"tb-model1\")",
   "id": "73e4941085a7ef02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Model",
   "id": "eba22008d42c4037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    # Creates embedded vector for each word-index\n",
    "    # (batch_size, steps) -> (batch_size, steps, features)\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vectorize_layer.vocabulary_size(),\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "    # Regularize by randomly dropping dimensions in feature vector\n",
    "    tf.keras.layers.SpatialDropout1D(0.5),\n",
    "    # LSTM Layer\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "    # Dense Layer\n",
    "    tf.keras.layers.Dense(\n",
    "        units=32,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.0001)\n",
    "    ),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_scheduler),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ],
   "id": "f0c4ae655e91d77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fit Model",
   "id": "b19b31352e3e6f0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history1 = model1.fit(\n",
    "    train_ds,\n",
    "    epochs=15,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3, verbose=1),\n",
    "        tf.keras.callbacks.TensorBoard(MODEL1_LOGS, histogram_freq=1)\n",
    "    ],\n",
    "    validation_data=val_ds,\n",
    "    verbose=2\n",
    ")"
   ],
   "id": "9e335b68ebe677aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluate Model",
   "id": "267b89676570b4f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = model1.evaluate(test_ds)\n",
    "\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ],
   "id": "ff369cd755ae444c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tb1 = program.TensorBoard()\n",
    "tb1.configure(argv=[None, '--load_fast', 'false', '--logdir', MODEL1_LOGS])\n",
    "url = tb1.launch()\n",
    "print(f\"TensorBoard listening on {url}\")"
   ],
   "id": "950f3de587bc0190",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outcome",
   "id": "3b3b9c97dc628768"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric=\"loss\", smoothing_std=10)\n",
    "plotter.plot({\n",
    "    \"CNN\": history0,\n",
    "    \"GRU\": history1\n",
    "})"
   ],
   "id": "9684b2957f532da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test model on a couple of example\n",
    "examples = tf.constant([\n",
    "  \"The movie was great! The animation and the graphics were out of this world.\",\n",
    "  \"The movie was amazing! I would recommend this movie.\",\n",
    "  \"The movie was terrible. I wouldn't recommend this movie.\"\n",
    "])\n",
    "\n",
    "predictions = model0.predict(examples)\n",
    "print(\"CNN:\")\n",
    "print([\"+\" if p > 0.0 else \"-\" for p in predictions])\n",
    "print()\n",
    "print(\"GRU:\")\n",
    "print([\"+\" if p > 0.0 else \"-\" for p in predictions])"
   ],
   "id": "2f8ce0e97e615d9a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
