{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Wake word detection (#2)\n",
    "\n",
    "Detecting wake word in audio sample. Each audio sample must have `SAMPLE_LEN` length in milliseconds.\n",
    "\n",
    "* Prepare dataset by applying positive or negative samples on top of background (noise)\n",
    "* Extract MFCC features from each training example\n",
    "* Create and fit binary classification models using `x` (MFCC) and `y` as `0` or `1`"
   ],
   "id": "77ad1d60bee8fc4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from wwd import load_raw_audios, generate_audio\n",
    "from common import CV_DATA_DIR\n",
    "from IPython.display import Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import librosa.core\n",
    "import librosa.feature"
   ],
   "id": "5d3a48ddc2501459",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TF Version: {tf.__version__}\")\n",
    "print(f\"TF Devices: {[d.device_type for d in tf.config.list_physical_devices()]}\")"
   ],
   "id": "ad2730886f60c5f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Parameters__",
   "id": "a10bd63607133f53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The rate of the audio\n",
    "SAMPLE_RATE = 16000\n",
    "# The length in ms of the audio (the segment to analyse for wake word)\n",
    "SAMPLE_LEN = 1000\n",
    "# The number of MFCC frames (depends on the length of the audio)\n",
    "MFCC_FRAMES = 32\n",
    "# The number of MFCC features to extract from the audio\n",
    "MFCC_FEATURES = 13"
   ],
   "id": "101eff4246e50eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "WWD_ROOT = os.path.join(CV_DATA_DIR, \"audio\", \"wwd\")\n",
    "WWD_PLAY = os.path.join(CV_DATA_DIR, \"playground\", \"audio\", \"wwd2\")"
   ],
   "id": "3fad6c607b333ee6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing datasets\n",
    "\n",
    "Source: [Open Voice OS](https://huggingface.co/datasets/OpenVoiceOS/synthetic-wakewords)"
   ],
   "id": "c06272c0edb70d86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Load audio files__",
   "id": "72f29b15ccac9056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "backgrounds_paths = [\n",
    "    os.path.join(WWD_ROOT, \"wake_word_noise\"),\n",
    "]\n",
    "positive_paths = [\n",
    "    os.path.join(WWD_ROOT, \"synthetic-wakewords\", \"hey_jarvis\")\n",
    "]\n",
    "negative_paths = [\n",
    "    os.path.join(WWD_ROOT, \"synthetic-wakewords\", \"yes\"),\n",
    "    os.path.join(WWD_ROOT, \"synthetic-wakewords\", \"no\")\n",
    "]"
   ],
   "id": "82439724d0148956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load background audio samples (limit each background by `SAMPLE_LEN`)\n",
    "backgrounds = load_raw_audios(\n",
    "    backgrounds_paths,\n",
    "    max_len=SAMPLE_LEN,\n",
    "    min_len=SAMPLE_LEN)\n",
    "# Load positive audio samples\n",
    "positives = load_raw_audios(positive_paths)\n",
    "# Load negative audio samples\n",
    "negatives = load_raw_audios(negative_paths)"
   ],
   "id": "5a1f30643dc6f5f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The number of positive samples\n",
    "BACKGROUNDS_NUM = len(backgrounds)\n",
    "# The number of positive samples\n",
    "POSITIVES_NUM = len(positives)\n",
    "# The number of negative samples\n",
    "NEGATIVES_NUM = len(negatives)\n",
    "\n",
    "print(f\"Background samples: {BACKGROUNDS_NUM}\")\n",
    "print(f\"Positive samples: {POSITIVES_NUM}\")\n",
    "print(f\"Negative samples: {NEGATIVES_NUM}\")"
   ],
   "id": "459b714e83eac5b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Process audio file__",
   "id": "b305b4c9ae0dec26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_audio(file_path, sr=SAMPLE_RATE, n_mfcc=MFCC_FEATURES, max_pad_len=None):\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    # Compute MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    # Transpose MFCC features to have time as the first dimension\n",
    "    mfccs = mfccs.T\n",
    "    # Pad or truncate to max_pad_len\n",
    "    if max_pad_len is not None:\n",
    "        if mfccs.shape[0] > max_pad_len:\n",
    "            mfccs = mfccs[:max_pad_len, :]\n",
    "        else:\n",
    "            pad_width = ((0, max_pad_len - mfccs.shape[0]), (0, 0))\n",
    "            mfccs = np.pad(mfccs, pad_width, mode='constant')\n",
    "    return mfccs"
   ],
   "id": "774904ce15475b02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EXAMPLE_PATH = os.path.join(\"data\", \"example.wav\")\n",
    "\n",
    "positive_num, _ = generate_audio(\n",
    "    EXAMPLE_PATH,\n",
    "    backgrounds,\n",
    "    positives,\n",
    "    negatives,\n",
    "    sample_len=SAMPLE_LEN,\n",
    "    max_negatives=2\n",
    ")\n",
    "\n",
    "is_positive = bool(positive_num > 0)\n",
    "print(f\"Is positive sample: {is_positive}\")"
   ],
   "id": "e63b339d225aefe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Play an example\n",
    "Audio(EXAMPLE_PATH)"
   ],
   "id": "2224a072edfc804a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get MFCC of one example\n",
    "mfcc = process_audio(EXAMPLE_PATH)\n",
    "\n",
    "# Adjust MFCC_FRAMES/MFCC_FEATURES parameters according to audio file length\n",
    "print(\"MFCC parameters:\")\n",
    "print(f\".....frames={mfcc.shape[0]}\")\n",
    "print(f\"...features={mfcc.shape[1]}\")"
   ],
   "id": "396e7867e67de041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert mfcc.shape[0] == MFCC_FRAMES, \\\n",
    "    \"The number of MFCC frames is invalid\"\n",
    "assert mfcc.shape[1] == MFCC_FEATURES, \\\n",
    "    \"The number of MFCC features is invalid\""
   ],
   "id": "8c512ba5cc9c30ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Generate examples__",
   "id": "64e0052604dadedf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The number of examples to generate\n",
    "n_samples = 50000\n",
    "\n",
    "OUTPUT_PATH = os.path.join(\"data\", \"example.wav\")\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, n_samples):\n",
    "    positive_num, _ = generate_audio(\n",
    "        OUTPUT_PATH,\n",
    "        backgrounds,\n",
    "        positives,\n",
    "        negatives,\n",
    "        sample_len=SAMPLE_LEN,\n",
    "        max_negatives=2)\n",
    "    x = process_audio(OUTPUT_PATH, max_pad_len=MFCC_FRAMES)\n",
    "    y = 1 if positive_num > 0 else 0\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ],
   "id": "ac9b0de7eec71fbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")"
   ],
   "id": "d992c5b2d8bc356b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert X.shape[1] == MFCC_FRAMES, \\\n",
    "    \"The number of MFCC frames is invalid\"\n",
    "assert X.shape[2] == MFCC_FEATURES, \\\n",
    "    \"The number of MFCC features is invalid\""
   ],
   "id": "eb214971cd0e1953",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save all examples to a file\n",
    "SAVE_DIR = os.path.join(WWD_PLAY, \"XY\")\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "np.save(os.path.join(SAVE_DIR, \"X.npy\"), X)\n",
    "np.save(os.path.join(SAVE_DIR, \"Y.npy\"), Y)"
   ],
   "id": "e236dec811d81f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Load examples__",
   "id": "4190e9c3cf4500be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load examples from a file\n",
    "X = []\n",
    "Y = []\n",
    "X_path = os.path.join(WWD_PLAY, \"XY\", \"X.npy\")\n",
    "if os.path.exists(X_path):\n",
    "    X = np.load(X_path)\n",
    "else:\n",
    "    print(f\"File <{X_path}> not found\")\n",
    "Y_path = os.path.join(WWD_PLAY, \"XY\", \"Y.npy\")\n",
    "if os.path.exists(Y_path):\n",
    "    Y = np.load(Y_path)\n",
    "else:\n",
    "    print(f\"File <{Y_path}> not found\")"
   ],
   "id": "a2e25d990cf364e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")"
   ],
   "id": "e85ab44228df8ecc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert X.shape[1] == MFCC_FRAMES, \\\n",
    "    \"The number of MFCC frames is invalid\"\n",
    "assert X.shape[2] == MFCC_FEATURES, \\\n",
    "    \"The number of MFCC features is invalid\""
   ],
   "id": "bc3fd6ff55b703e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Split example into different sets__",
   "id": "1e4a5f150bbbd2d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split all examples into different sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_valid.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ],
   "id": "f2b86ae32331e12f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build and train model",
   "id": "d7cfeb9b4cc5f624"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_pred_classes,\n",
    "        average='binary')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "258f9e1b6cb2b60b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM-based Model",
   "id": "1ca14add3e0b978e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_lstm_model(input_shape=(MFCC_FRAMES, MFCC_FEATURES)):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        # 1st layer\n",
    "        tf.keras.layers.Conv1D(filters=64,kernel_size=3, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        # 2nd layer\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        # 3rd layer\n",
    "        tf.keras.layers.Conv1D(256, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        # 4th layer\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        # 5th layer\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ],
   "id": "7dbfe00b97d6ddc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lstm_model = create_lstm_model()\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ],
   "id": "45fabde0340a94b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lstm_model.summary()",
   "id": "194e8b08f4690030",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    callbacks=callbacks,\n",
    ")"
   ],
   "id": "8a4aeab8a0eff5a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save model into file\n",
    "lstm_model.save(os.path.join(WWD_PLAY, \"models\", \"model-lstm.keras\"))"
   ],
   "id": "9768dbaa6c7186ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "__Load model from file__\n",
    "\n",
    "(load model from previously saved file to skip the fitting again)"
   ],
   "id": "6ab289521fe0a24e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lstm_model = tf.keras.models.load_model(os.path.join(WWD_PLAY, \"models\", \"model-lstm.keras\"))",
   "id": "79905223c436186d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Evaluate model__",
   "id": "aa8e90db53504829"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate model o na test set\n",
    "evaluate_model(lstm_model, X_test, Y_test)"
   ],
   "id": "5800afe9aa58a8bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GRU-based Model",
   "id": "457ec7fb07514eda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_gru_model(input_shape=(MFCC_FRAMES, MFCC_FEATURES)):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        # 1st layer\n",
    "        tf.keras.layers.Conv1D(filters=64,kernel_size=3, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        # 2nd layer\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        # 3rd layer\n",
    "        tf.keras.layers.Conv1D(256, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        # 4th layer\n",
    "        tf.keras.layers.GRU(128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.GRU(64),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        # 5th layer\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ],
   "id": "e9eb8361feff8b7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gru_model = create_gru_model()\n",
    "\n",
    "gru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ],
   "id": "520dadaa5200c9ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gru_model.summary()",
   "id": "628de148984ba94d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = gru_model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    callbacks=callbacks,\n",
    ")"
   ],
   "id": "4def8c9c95d2e93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save model into file\n",
    "gru_model.save(os.path.join(WWD_PLAY, \"models\", \"model-gru.keras\"))"
   ],
   "id": "907d3ff66b07ed36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "__Load model from file__\n",
    "\n",
    "(load model from previously saved file to skip the fitting again)"
   ],
   "id": "b72349f72d5f409f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gru_model = tf.keras.models.load_model(os.path.join(WWD_PLAY, \"models\", \"model-gru.keras\"))",
   "id": "e60414e71b2c141e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "__Evaluate model__",
   "id": "12a659e9b1e9796f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_model(gru_model, X_test, Y_test)",
   "id": "35d8d2d7df94e675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predict",
   "id": "1b12601ea35ce527"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List all input audio devices\n",
    "devices = sd.query_devices()\n",
    "print(devices)"
   ],
   "id": "6c5b4c04931e375a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select audio device to use by the index\n",
    "sd.default.device = 8"
   ],
   "id": "de2c657e62976741",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Record audio\n",
    "duration = SAMPLE_LEN / 1000\n",
    "\n",
    "print(\"Recording...\")\n",
    "sample_audio = sd.rec(\n",
    "    int(SAMPLE_RATE * duration),\n",
    "    samplerate=SAMPLE_RATE,\n",
    "    channels=1,\n",
    "    dtype='int16')\n",
    "sd.wait()\n",
    "print(\"Recording finished.\")\n",
    "write(os.path.join(\"data\", \"example.wav\"), SAMPLE_RATE, sample_audio)"
   ],
   "id": "779e3ddbdd4c9962",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audio, sr = librosa.load(os.path.join(\"data\", \"example.wav\"), sr=SAMPLE_RATE)\n",
    "mfcc = process_audio(os.path.join(\"data\", \"example.wav\"), max_pad_len=MFCC_FRAMES)\n",
    "mfcc = np.expand_dims(mfcc, axis=0)\n",
    "print(f\"MFCC shape: {mfcc.shape}\")"
   ],
   "id": "db34648a0f083565",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL1_PATH = os.path.join(WWD_PLAY, \"models\", \"model-lstm.keras\")\n",
    "MODEL2_PATH = os.path.join(WWD_PLAY, \"models\", \"model-gru.keras\")\n",
    "\n",
    "# Select which model to use: LSTM (MODEL1_PATH) or GRU (MODEL2_PATH)\n",
    "model = tf.keras.models.load_model(MODEL2_PATH)\n",
    "\n",
    "prediction = model.predict(mfcc)"
   ],
   "id": "102b958564a2081e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Prediction: {prediction[0][0] * 100:.2f}%\")\n",
    "print(f\"Positive: {prediction[0][0] > 0.5}\")"
   ],
   "id": "596bef86f167d0ed",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
